<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hi, Hexo | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Boston Dynamics 踹不倒的机器狗 Boston Dynamics 跳1.2米轮式机器人Handle Deep MindDQN –&amp;gt; AtariA3C &amp;amp; UNREAL –&amp;gt; AlphaGo 深度增强学习：走向通用人工智能之路开源增强学习平台综述 马尔可夫决策过程MDP1. 基础概念状态集合S: 有限状态state集合，$s$表示某个特定状态 动作集合A: 有限动作">
<meta property="og:type" content="article">
<meta property="og:title" content="Hi, Hexo">
<meta property="og:url" content="http://yoursite.com/2018/09/13/Hi-Hexo/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Boston Dynamics 踹不倒的机器狗 Boston Dynamics 跳1.2米轮式机器人Handle Deep MindDQN –&amp;gt; AtariA3C &amp;amp; UNREAL –&amp;gt; AlphaGo 深度增强学习：走向通用人工智能之路开源增强学习平台综述 马尔可夫决策过程MDP1. 基础概念状态集合S: 有限状态state集合，$s$表示某个特定状态 动作集合A: 有限动作">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://n.sinaimg.cn/tech/transform/20160303/7Shv-fxqaser8990878.jpg">
<meta property="og:image" content="http://static.leiphone.com/uploads/new/article/740_740/201703/58b6dfdf24ca0.gif">
<meta property="og:image" content="http://static.leiphone.com/uploads/new/article/740_740/201702/58aa65b77d81d.png?imageMogr2/format/jpg/quality/90">
<meta property="og:image" content="http://7xkmdr.com1.z0.glb.clouddn.com/rl2_1.png">
<meta property="og:image" content="http://7xkmdr.com1.z0.glb.clouddn.com/rl2_2.png">
<meta property="og:image" content="http://7xkmdr.com1.z0.glb.clouddn.com/rl2_3.png">
<meta property="og:image" content="http://7xkmdr.com1.z0.glb.clouddn.com/rl2_4.png">
<meta property="og:image" content="http://7xkmdr.com1.z0.glb.clouddn.com/rl2_5.png">
<meta property="og:image" content="http://7xkmdr.com1.z0.glb.clouddn.com/rl2_6.png">
<meta property="og:updated_time" content="2018-09-13T16:06:48.989Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hi, Hexo">
<meta name="twitter:description" content="Boston Dynamics 踹不倒的机器狗 Boston Dynamics 跳1.2米轮式机器人Handle Deep MindDQN –&amp;gt; AtariA3C &amp;amp; UNREAL –&amp;gt; AlphaGo 深度增强学习：走向通用人工智能之路开源增强学习平台综述 马尔可夫决策过程MDP1. 基础概念状态集合S: 有限状态state集合，$s$表示某个特定状态 动作集合A: 有限动作">
<meta name="twitter:image" content="http://n.sinaimg.cn/tech/transform/20160303/7Shv-fxqaser8990878.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Hi-Hexo" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/13/Hi-Hexo/" class="article-date">
  <time datetime="2018-09-13T15:17:10.000Z" itemprop="datePublished">2018-09-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Hi, Hexo
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>Boston Dynamics</strong> 踹不倒的机器狗<br><img src="http://n.sinaimg.cn/tech/transform/20160303/7Shv-fxqaser8990878.jpg" alt="2016年的机器狗"></p>
<p><strong>Boston Dynamics</strong> 跳1.2米轮式机器人Handle<br><img src="http://static.leiphone.com/uploads/new/article/740_740/201703/58b6dfdf24ca0.gif" alt="2017年的轮式机器人Handle"></p>
<p><strong>Deep Mind</strong><br><em>DQN</em> –&gt; Atari<br><em>A3C</em> &amp; <em>UNREAL</em> –&gt; AlphaGo<br><img src="http://static.leiphone.com/uploads/new/article/740_740/201702/58aa65b77d81d.png?imageMogr2/format/jpg/quality/90" alt="AlphaGo"></p>
<h2 id="深度增强学习：走向通用人工智能之路"><a href="#深度增强学习：走向通用人工智能之路" class="headerlink" title="深度增强学习：走向通用人工智能之路"></a><a href="https://www.zybuluo.com/tinadu/note/629229" target="_blank" rel="noopener">深度增强学习：走向通用人工智能之路</a></h2><p>开源增强学习平台综述</p>
<h2 id="马尔可夫决策过程MDP"><a href="#马尔可夫决策过程MDP" class="headerlink" title="马尔可夫决策过程MDP"></a>马尔可夫决策过程MDP</h2><h3 id="1-基础概念"><a href="#1-基础概念" class="headerlink" title="1. 基础概念"></a>1. 基础概念</h3><p><strong>状态集合S</strong>: 有限状态state集合，$s$表示某个特定状态</p>
<p><strong>动作集合A</strong>: 有限动作action集合，$a$表示某个特定动作</p>
<p><strong>状态转移矩阵P</strong>: 矩阵每一项是从S中一个状态$s$转移到另一个状态${s’}$的概率$P_{ss′}=P[S_{t+1}=s′|S_t=s]$以及执行动作$a$后从一个状态转移到另一个概率为$P^a_{ss′}=P[S_{t+1}=s′|S_t=s,A_t=a]$。这里的状态转移矩阵决定了马尔可夫性质，即未来状态只与当前状态有关而与过去状态无关。矩阵一行之和为1。</p>
<p><strong>策略π</strong>: 状态$s$下执行动作$a$的概率，$π(a|s)=P[A_t=a|S_t=s]$</p>
<p><strong>reward函数ER</strong>: 这个函数是immediate reward的期望，即在时刻t的时候，agent执行某个action后下一个时刻立即能得到的reward $R_{t+1}$的期望，它由当前的状态决定。状态$s$下immediate reward期望为$ER_s=E[R_{t+1}|S_t=s]$，状态$s$下执行动作$a$后immediate reward期望为$ER^a_as=E[R_{t+1}|S_t=s,A_t=a]$</p>
<p><strong>Return G_t与discount γ</strong>: G_t是t时刻之后未来执行一组action能够获得的reward，即、、t+1、t+2、t+3…未来所有时刻reward之和，是未来时刻reward在当前时刻的体现，但是越往后的时刻它能反馈回来的reward需要乘以一个discount系数，系数γ∈[0,1]会产生一个打折的效果，这是因为并没有一个完美的模型能拟合出未来会发生什么，未来具有不确定性，同时这样计算会方便，避免了产生状态的无限循环，在某些情况下，即时产生的reward即$R_{t+1}会比未来时刻更值得关注，符合人的直觉。因此会比未来时刻更值得关注，符合人的直觉。因此G_t=R_{t+1}+γR_{t+2}+…=\sum^∞<em>{k=0}γ^kR</em>{t+k+1}$</p>
<p><strong>状态值函数$v(s)$</strong>: 即基于$t$时刻的状态$s$能获得的return的期望，$v(s)=E[G_t|S_t=s]$，这里是仅按照状态转移矩阵选择执行何种动作，如果加入动作选择策略，那么函数就变成了$v_π(s)=E_π[G_t|S_t=s]$</p>
<p><strong>动作值函数$q_π(s,a)$</strong>: 基于t时刻的状态s，选择特定的一个action后能获得的return期望，这里的选择过程就隐含加入了策略。$q_π(s,a)=E_π[G_t|S_t=s,A_t=a]$<br>​    </p>
<h3 id="2-MDP与实例分析"><a href="#2-MDP与实例分析" class="headerlink" title="2. MDP与实例分析"></a>2. MDP与实例分析</h3><h4 id="马尔可夫链-过程"><a href="#马尔可夫链-过程" class="headerlink" title="马尔可夫链/过程"></a><strong>马尔可夫链/过程</strong></h4><p><strong>马尔可夫链/过程(Markov Chain/Process)</strong>，是具有markov性质的随机状态$s_1,s_2,…$序列。由$[S,P]$组成。如下图圆圈内是状态，箭头上的值是状态之间的转移概率。class是指上第几堂课，facebook指看facebook网页，pub指去酒吧，pass指通过考试，sleep指睡觉。例如处于class1有0.5的概率转移到class2，或者0.5的概率转移到facebook。<br>​    <img src="http://7xkmdr.com1.z0.glb.clouddn.com/rl2_1.png" alt="马尔可夫链/过程"><br>从而可以产生非常多的随机序列，例如C1 C2 C3 Pass Sleep或者C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep等。这些随机状态的序列就是马尔可夫过程。这里可以看到有一些状态发生了循环。</p>
<h4 id="马尔可夫奖赏过程"><a href="#马尔可夫奖赏过程" class="headerlink" title="马尔可夫奖赏过程"></a><strong>马尔可夫奖赏过程</strong></h4><p><strong>马尔可夫奖赏过程(Markov Reward Process)</strong>，即马尔可夫过程加上value judgement，value judegment即判断一个像上面一个特定的随机序列有多少累积reward，也就是计算出$v(s)$。它由$[S,P,R,γ]$组成，示意图如下<br><img src="http://7xkmdr.com1.z0.glb.clouddn.com/rl2_2.png" alt="马尔可夫奖赏过程"><br>可以看出比图1多了红色部分即$R$，但是$R$的取值只决定了immediate reward，在实际过程中肯定是需要考虑到后面步骤的reward才能确定当前的选择是否正确。而实际上$v(s)$由两部分组成，一个是immediate reward，一个是后续状态产生的discounted reward，推导如下(这里我觉得视频里似乎把$ER_s$与$R_{t+1}$的取值当成一样的了)，推导出来的这个式子称为Bellman方程。<br>$$<br>\begin{equation}<br>\begin{split}<br>v(s)=&amp;E[G_t|S_t=s]\<br>=&amp;E[R_{t+1}+γ(R_{t+2}+γR_{t+3}+…)|S_t=s]\<br>=&amp;E[R_{t+1}+γG_{t+1}|S_t=s]\<br>=&amp;ER_s+γ\sum_{s′∈S}P_{ss′}v(s′)<br>\end{split}<br>\end{equation}<br>$$<br>那么每一个状态下能得到的状态值函数取值或者说累积reward如下所示，即原来写着class、sleep状态的地方替换成了数字(这里假设\gamma =1\gamma =1)。可以从sleep状态出发，推导出每个状态的状态值函数取值，如右上角红色公式所示。最右的-23与-13，列出二元一次方程组即可求出。<br><img src="http://7xkmdr.com1.z0.glb.clouddn.com/rl2_3.png" alt="State Value Function Example"><br>将Bellman方程表达成矩阵形式，变成了v=ER+\gamma P_vv=ER+\gamma P_v，是个线性等式，直接求解得到v=(I−γP)^{−1}ERv=(I−γP)^{−1}ER。而这样求解的话计算复杂度是O(n^3)O(n^3)，所以一般通过动态规划、蒙特卡洛估计与Temporal-Difference learning这些迭代的方式求解。</p>
<h4 id="马尔可夫决策过程"><a href="#马尔可夫决策过程" class="headerlink" title="马尔可夫决策过程"></a><strong>马尔可夫决策过程</strong></h4><p><em>马尔可夫决策过程(Markov Decision Process)</em>，它是拥有决策能力的马尔可夫奖赏过程，个人理解是MRP是将所有情况都遍历，而MDP则是选择性的遍历某些情况。它由$[S,A,P,R^a_s,γ,π(a|s)]$组成，并且拥有两个值函数$v_π(s)$和$q_π(s,a)$。根据这两个值函数的定义，它们之间的关系表示为$v_π(s)=∑<em>{a∈A}π(a|s)q</em>π(s,a)$以及$q_π(s,a)=R^a_s+\gamma∑<em>{s′∈S}P^a</em>{ss′}v_π(s′)$。第二个式子是说当选择一个action之后，转移到不同状态下之后获取的reward之和是多少。将两个式子互相代入，可以得到如下的Bellman期望方程。<br>$$<br>v_π(s)=\sum_{a∈A}π(a|s)(R^a_s+\gamma∑<em>{s′∈S}P^a</em>{ss′}v_π(s′))<br>$$</p>
<p>$$<br>q_π(s,a)=R^a_s+\gamma \sum_{s′∈S}P^a_{ss′}\sum_{a′∈A}π(a′|s′)q_π(s′,a′)<br>$$</p>
<p>下图是一个MDP的例子，箭头上的单词表示action，与MRP不同的是，这里给出的immediate reward是同时在某个状态$s$和某个动作$a$条件下，所以图中$R$不是只取决于$s$，而是取决于$s$和$a$。右上角的等式表达出了这一个状态的状态值函数求解过程。<br><img src="http://7xkmdr.com1.z0.glb.clouddn.com/rl2_4.png" alt="Markov Decision Processes Example"></p>
<p>由于策略$π(a|s)$是可以改变的，因此两个值函数的取值不像MRP一样是固定的，那么就能从不同的取值中找到一个最大值即最优值函数(这节课没有讲如何求解)。例如下面两个图就是上面的例子能找到的最优状态值函数$v_∗(s)$与最优动作值函数$q_∗(s,a)$。如果知道了$q_∗(s,a)$，那么也就知道了在每一步选择过程中应该选择什么样的动作。也就是说MDP需要解决的问题并不是每一步到底会获得多少累积reward，而是找到一个最优的解决方案。这两个最优值函数同样存在着一定关系，$v_∗(s)=\max \limits_aq_∗(s,a)$，从而可以推出$q_∗(s,a)=R^a_s+\gamma \sum_{s′∈S}P^a_{ss′}\max\limits_{a′}q_∗(s′,a′)$，这个等式称为Bellman优化方程，它不是一个线性等式，没有闭式解。通常通过值迭代、策略迭代、Q-learning、Sarsa等方法求解。<br>$$<br>v_∗(s)=\max\limits_aq_∗(s,a)<br>$$</p>
<p>$$<br>q_∗(s,a)=R^a_s+\gamma\sum_{s′∈S}P^a_{ss′}\max\limits_{a′}q_∗(s′,a′)<br>$$</p>
<p><img src="http://7xkmdr.com1.z0.glb.clouddn.com/rl2_5.png" alt="Optimal State-Value Function Example"></p>
<p><img src="http://7xkmdr.com1.z0.glb.clouddn.com/rl2_6.png" alt="Optimal Action-Value Function Example"></p>
<p>实际上一定存在这样的一个最优策略$π<em>∗$。可以通过最大化$q</em>∗(s,a)$获得。针对任意的MDP问题，总是存在一个最优的deterministic policy。<br>​    </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/09/13/Hi-Hexo/" data-id="cjm0pzv7n0001u4q7fhg61oe5" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2018/09/13/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/09/13/Hi-Hexo/">Hi, Hexo</a>
          </li>
        
          <li>
            <a href="/2018/09/13/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>