title: 集成学习之Bagging和Boosting
author: Mophei
tags:
  - 机器学习
categories:
  - 机器学习
mathjax: true
date: 2018-10-16 19:01:00
---
集成学习(Ensemble Learning)有两个流派，一个是boosting派系，它的特点是各个弱学习器之间有依赖关系。另一种是bagging流派，它的特点是各个弱学习器之间没有依赖关系，可以并行拟合。

<!--more-->

Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法。即将弱分类器组装成强分类器的方法。

首先介绍Bootstraping，即自助法：它是一种有放回的抽样方法（可能抽到重复的样本）。

1、Bagging (bootstrap aggregating)

Bagging即套袋法，其算法过程如下：
A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）
 
2、Boosting
其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。
关于Boosting的两个核心问题：
1）在每一轮如何改变训练数据的权值或概率分布？
通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。
2）通过什么方式来组合弱分类器？
通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。
而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。
 
3、Bagging，Boosting二者之间的区别
Bagging和Boosting的区别：
1）样本选择上：
Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
2）样例权重：
Bagging：使用均匀取样，每个样例的权重相等
Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
3）预测函数：
Bagging：所有预测函数的权重相等。
Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
4）并行计算：
Bagging：各个预测函数可以并行生成
Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。
 
4、总结
这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。
下面是将决策树与这些算法框架进行结合所得到的新的算法：
1）Bagging + 决策树 = 随机森林
2）AdaBoost + 决策树 = 提升树
3）Gradient Boosting + 决策树 = GBDT

## 随机森林（RF, Random Forest）
### 原理
Random Forest（随机森林）是Bagging的扩展变体，它在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入随机特征选择，因此可以概括RF包括四个部分：
 1. 随机选择样本（放回抽样）
 2. 随机选择特征
 3. 构建决策树
 4. 随机森林投票（平均）

随机选择样本和Bagging相同，随机选择特征是指在书的构建中，会从样本集的特征集合中随机选择部分特征，然后再从这个 子集中选择最优的属性用于划分，这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵随机树），但是由于随机森林的‘平均’特性，会使得它的方差减小，而且方差的减小步长了偏差的增大，因此总体而言是更好的模型。

在构建决策树的时候，RF的每棵决策树都最大可能的进行生长而不进行剪枝；在对预测输出进行结合时，RF通常对分类问题使用简单投票法，回归任务使用简单平均法。

RF的重要特性是不用对其进行交叉验证或者使用一个独立的测试集获得无偏估计，它可以在内部进行评估，也就是说在生成的过程中可以对误差进行无偏估计，由于每个基学习器只使用了训练集中约63.2%的样本，剩下约36.8%的样本可用做验证集来对其泛化性能进行“包外估计”

对于一个样本，它在某一次含m个样本的训练集的随机采样中，每次被采集到的概率是$\frac{1}{m}$。不被采集到的概率为$1- \frac{1}{m}$。如果m次采样都没有被采集中的概率是$(1- \frac{1}{m})^m$。当$m \to \infty $时，$(1-\frac{1}{m})^m \to \frac{1}{e} \simeq 0.368$。也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。
对于这部分大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。[^刘建平博客]

RF和Bagging对比：RF的起始性能较差，特别当只有一个基学习器时，随着学习器数据增多，随机森林通常会收敛到更低的泛化误差。随机森林的训练效率也会高于Bagging，因为在单个决策树的构建中，Bagging使用的是‘确定性’决策树，在选择特征划分结点时，要对所有的特征进行考虑，而随机森林使用的是‘随机性’特征数，只需考虑特征的子集。
### 优缺点
随机森林的优点较多，简单总结：
 1. 在数据集上表现良好，相对于其他算法有较大的优势（训练速度、预测准确度）
 2. 能够处理很高维的数据，并且不用特征选择，而且在训练完成后，给出特征的重要性
 3 容易做成并行化方法

RF的缺点：在噪声较大的分类或者回归问题上的会过拟合

## GBDT
提GBDT之前，谈下Boosting，Boosting是一种与Bagging很类似的技术。不论是Boosting还是Bagging，所使用的多个分类器类型都是一致的。但是在Boosting中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。
由于Boosting分类的结果是基于所有分类器的加权求和结果的，因此Boosting与Bagging不太一样，Bagging中的分类器权值是一样的，而**Boosting中的分类器权重并不相等，每个权重代表对应的分类器在上一轮迭代中的成功度。**
### 原理
GBDT与传统的Boosting区别较大。GBDT的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向上建立模型，所以说，在Gradient Boost中，每个新模型的建立是为了使得之前的模型的残差往梯度下降的方法，与*传统的Boosting中关注正确错误的样本加权有着很大的区别*。

在Gradient Boosting算法中，关键就是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树。

GBDT会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树而不是分类树（尽管GBDT调整后也可以用于分类，单不代表GBDT的树为分类树）

### 优缺点
GBDT的性能在RF的基础上又有一步提升，因此其优点也很明显：
 1. 它能灵活地处理各种类型的数据
 2. 在相对较少的调参时间下，预测的准确度较高

当然，由于它是Boos，因此基学习器之前存在串行关系，难以并行训练数据。

## XGBoost
### 原理
XGBoost的性能在GBDT上又有一步提升，而其性能也能通过各种比赛管窥一二。坊间对XGBoost最大的认知在于其能够自动地运用CPU的多线程进行并行计算，同时在算法精度上也进行了精度的提高。
由于GBDT在合理的参数设置下，旺旺要生成一定数量的树才能达到令人满意的准确率，当数据集较复杂时，模型可能需要几千次迭代运算。但是XGBoost利用并行的CPU更好的解决了这个问题。
其实XGBoost和GBDT的差别也较大，这一点也同样体现在其性能表现上，详见XGBoost与GBDT的区别。
## 区别
### GBDT和XGBoost区别
 1. 传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑回归（分类）或者线性回归
 2. 传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数；
 3. XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性
 4. shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）
 5. 列抽样。XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过拟合，还能减少计算
 6. 对缺失值的处理。对于特征的值有确实的样本，XGBoost还可以自动学习出它的分裂方向
 7. XGBoost工具支持并行。Boosting是一种串行的结构，怎么并行呢？注意XGBoost的并行是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。**XGBoost的并行是在特征粒度上的。**我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选择增益最大的那个特征去做分裂，那么哥哥特征的增益计算就可以开多个线程进行[^RF、GBDT、XGBoost面试级整理]



[^刘建平博客]: [Bagging与随机森林算法原理小结](https://www.cnblogs.com/pinard/p/6156009.html)
[^RF、GBDT、XGBoost面试级整理]: [RF、GBDT、XGBoost面试级整理](https://cloud.tencent.com/developer/article/1061955)
