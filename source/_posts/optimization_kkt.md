title: 优化问题中的KKT条件
mathjax: true
author: Mophei
tags:
  - 最优化问题
categories:
  - 机器学习
date: 2019-04-14 10:40:00
keywords:
description:
---
最优化，是应用数学的一个分支。最优化问题简单讲就是[^1]：

1. 构造一个合适的目标函数，是的这个目标函数取到极值的解就是你所要求的东西；
2. 找到一个能让这个目标函数取到极值的解的方法

[^1]: [最优化问题的简洁介绍是什么？](https://www.zhihu.com/question/26341871)

## 初识最优化问题

主要研究以下形式的问题：

​	给定一个函数$f: A \rightarrow \mathbb{R}​$，寻找一个元素$X^0 \in A​$ 使得对于所有$A​$ 中的$X​$ , $f(X^0) \leq f(X)​$ (最小化)；或者$f(X^0) \geq f(X)​$ (最大化)。

这类问题也被称作“数学规划”，例如线性规划。典型的，$A$ 一般为欧几里得空间$\mathbb{R}^n$ 中的子集，通常由一个$A$ 必须满足的约束等式或者不等式来规定。$A$ 的元素被称为可行解。函数$f​$ 被称为目标函数，或者代价函数。一个最小化（或者最大化）目标函数的可行解被称为最优解[^维基最优化]。

## 主要分支

最优化问题主要有以下几类分支[^2]

- **线性规划：**当目标函数是线性函数而且集合$A$ 是由线性等式函数和线性不等式函数来确定时，称这类问题为线性规划
- **整数规划：**当线性规划问题的部分或者所有的变量局限于整数值时，称为整数规划问题
- **二次规划：**目标函数是二次函数，而且集合$A​$ 必须是由线性等式和线性不等式函数来确定的
- **分数规划：**研究的是如何优化两个非线性函数的比例
- **非线性规划：**研究的目标函数或是限制函数中含有非线性函数的问题
- **随机规划：**研究的是某些变量是随机变量的问题
- **动态规划：**研究的是最优策略基于将问题分解成若干个较小的子问题的优化问题
- **组合最优化：**研究的是可行解离散或者是可转化为离散的问题
- **无限维最优化：**研究的是可行解的集合是无限维空间的子集的问题，一个无限维空间的立即是函数空间

[^2]: [最优化](https://zh.wikipedia.org/wiki/%E6%9C%80%E4%BC%98%E5%8C%96)


## 问题分类

基于约束条件的种类，最优化问题可以分为以下种类[^3]：

![约束分类.png](https://upload-images.jianshu.io/upload_images/2268630-114df62e5346bfb2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

基于目标函数的状态，最优化问题又可以分成：

![目标函数分类.jpg](https://upload-images.jianshu.io/upload_images/2268630-0d09368c044fd61b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 解法分类和选择

在实际工作中，如何选择最优化问题的解法呢[^3]？

![解法选择.jpg](https://upload-images.jianshu.io/upload_images/2268630-499eb184f219f259.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对应上图：

- 离散最优化方法：主要用于求解目标函数不连续或者不可导的情况，例如爬山法、模拟退火、遗传算法和蚁群算法等。
- 线性规划和二次规划：运筹学的重要研究内容，适用于目标函数是线性或者二次函数的形式
- 连续最优化方法：适用于逻辑回归、SVM、神经网络等机器学习问题，主要方法包括梯度下降、牛顿法和拟牛顿法

[^3]: [最优化问题-概述](https://zhuanlan.zhihu.com/p/22801652)


## KKT条件与凸优化的关系探讨

### 带约束优化问题的KKT条件

Karush-Kuhn-Tucker (KKT)条件是非线性规划(nonlinear programming)最佳解的必要条件。KKT条件将Lagrange乘数法(Lagrange multipliers)所处理涉及等式的约束优化问题推广至不等式。在实际应用上，KKT条件(方程组)一般不存在代数解，许多优化算法可供数值计算选用。KKT条件作为带约束可微分优化问题的最优性条件，有着非常重要的地位。

带约束优化问题的数学表述为：
$$
\begin{equation}
\begin{aligned}
\min_{x \in D} \ &f(x)\\
s.\ t. \ &g_i(x) \leq 0, \  i=1, \cdots , m\\
&h_j(x)=0, \ j=1, \cdots, n
\end{aligned}
\end{equation}
$$

- $f$ 被称为目标函数(objective or criterion function)
- $g$ 被称为不等式约束函数(inequality constraint function)
- $h$ 被称为等式约束函数(equality constraint function)
- 满足$g_i(x) \leq 0​$ 且 $h_j(x)=0​$ 的点称为可行解(feasible point)，可行解的集合称为可行域
- 在可行域中$f(x)$ 最小的点称为最优值(optimal value)， 记为 $f^*$
- 使得函数$f(x)$ 取得$f^*$ 的点$x$ 称为最优解(optimal point、solution或 minimizer)
- 如果点$x$ 满足$f(x) \leq f^* + \epsilon $ ，则称点$x$ 为$\epsilon - suboptimal$ 解

若目标函数和约束函数$f, g_1, \cdots, g_m, h_1, \cdots, h_n$ 可微且一阶导数连续，则该优化问题为带约束可微分优化问题。
  对于带约束可微分优化问题，存在非负实数$\mu_1, \cdots, \mu_m$ 和实数$\lambda_1, \cdots, \lambda_n$，若$x^*$ 为局部最优解，则以下条件成立：
$$
\bigtriangledown f(x^*) + \sum_{i=1}^m \mu_i^* \bigtriangledown g_i(x^*) + \sum_{j=1}^n \lambda_j^* \bigtriangledown h_j(x^*) = 0 \\
h_j(x^*) = 0,\ j=1, \cdots, n	\\
g_i(x^*) \leq 0, \ 	i=1, \cdots, m	\\
\mu_i^* \geq 0, \ i=1, \cdots, m 	\\
\mu_i^* g_i(x^*)=0, \ i=1, \cdots, m  	\\
$$

即KKT条件成立。一般情况下，**KKT最优性条件是带约束优化问题的必要条件**。

在KKT条件中有 $\mu_i^\* g_i(x^\*)=0$，则 $\mu_i^\* =0$ 或 $g_i(x^\*)=0$ 总成立。当 $\mu_i^\*=0$，则约束条件$g_i(x)$不起作用，也就是说此时$x^\*$ 并未到达边界上，因此即使去掉该约束也不会影响最优解的取值；当$g_i(x^\*)=0$ ，则该约束条件起作用，此时$x^\*​$ 在边界上。<!--此处是非支持向量可以去掉的原因，即仅与支持向量有关-->


事实上，上面陈述的KKT条件并不完全正确（严谨），还缺少一个regularity条件[^4]。

[^4]: [【学界】关于KKT条件的深入探讨](https://zhuanlan.zhihu.com/p/33229011)

### KKT最优性条件的重要性

我们经常会提到或用到最优性条件，那么为什么最优性条件如此重要呢？如果一个优化问题有最优性条件的话，那这个优化问题的性质实际上是比较好的。

 1. 通过最优性条件可以比较容易的验证任意的一个解是不是最优解。*例如 KKT条件，它是最优解的必要条件，它就可以把可行域里边很多的不是最优解的解轻松的排除掉，让我们仅仅在满足必要条件（KKT条件）的解里边进一步寻找真正的最优解。*
 2. 最优性条件可以指导算法的设计。例如对于无约束可微分的优化问题，我们采用梯度法，牛顿法，拟牛顿法等，其收敛性的证明都是证明最终算法能收敛到导数等于0的地方。所以算法的设计都是考虑如何能够收敛到最优性条件去，这样在很多情况下比直接去求解极值要容易的多。

### KKT条件和凸优化的关系

KKT主要是针对带约束的可微分的优化问题，凸优化[^5]研究的对象是目标函数为凸函数，约束为凸集的优化问题。因此这两者研究的对象，有交集，也各有不同。

第一类问题为两类问题的交集，即带约束的**可微分凸优化问题**。这类问题同时具备两类问题的性质，可微分和凸优化性质，让原来KKT从局部最优解的必要条件变为全局最优解的充要条件。

第二类问题是**凸优化但是不可微分**，这类问题也较为常见，在拉格朗日松弛算法中，对偶问题一般都是不可微分的凸优化问题，因为不可微分，传统的基于梯度的方法就不适用了，一般采用次梯度的方法，主要难点在于次梯度如何确定，由于次梯度不唯一，如何确定一个简单有效的次梯度也是一个问题。

第三类问题是**可微分但不是凸优化**，这类问题也很多，一般这类问题都可以采用基于梯度的算法来求解，例如对神经网络的训练多数就属于这类问题。采用梯度法仅仅能保证收敛到局部最优的必要条件而已。因此该类问题的受困于陷入鞍点和全局最优的寻找是很困难的。

[^5]: 凸优化问题相对优化问题的定义而言，要求函数$f(x)$ 和 $g_i(x)$ 是凸函数，$h_j(x)=a_j^Tx+b$ 是仿射函数($Ax+b=0$)

