title: 【读书笔记之】朴素贝叶斯法
mathjax: true
tags:
  - 机器学习
  - 算法
categories:
  - 机器学习
date: 2018-11-12 23:18:00
keywords:
description:
---
朴素贝叶斯（naive Bayes）法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入\输出的联合概率分布；然后基于此模型，对给定的输入$x$ ，利用贝叶斯定理求出后验概率最大的输出$y$。

<!--more-->

朴素贝叶斯具有以下优点：
 -  在测试数据集中预测很容易，也很快。在多类预测中也表现良好
 - 当独立性假设成立时，Naive Bayes分类器较逻辑回归等其他模型表现更好，需要的训练数据更少
 - 与数据变量相比，在特征为分类变量的情况下朴素贝叶斯表现良好。而对于数据变量，需要特征满足正态分布的假设（即钟型曲线，这是一个强假设）

同时朴素贝叶斯存在以下不足：
 - 如果分类标量具有在训练数据集中未观察到的类别（在测试数据集中），则模型将制定0（零）概率并且将无法进行预测，这通常被称为“零概率”。为了解决这个问题， 我们可以使用平滑技术。其中，最简单的平滑技术之一称为拉普拉斯估计。
 - 在另一方面朴素贝叶斯也被称为一个坏的估计，这样的概率输出形式predict_proba不应太认真对待。
 - 朴素贝叶斯的另一个限制是特征变量独立性的假设。 在现实生活中，我们几乎不可能得到一组完全独立的预测变量。

在具体学习朴素贝叶斯法之前，我们先回顾下几个概念：
 - **先验概率：**指根据以往经验和分析得到的概率，即在未知条件下对事件发生可能性猜测的数学表示
 - **后验概率：**指事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小

## 算法思想
朴素贝叶斯方法可以看成是以下流程的实现：
> 训练数据集——A——>联合概率分布$p(x,y)$——B——>预测类$y$
> A：特征条件独立
> B：最大化后验概率

## 求解方法
### 极大似然估计
### 贝叶斯估计
