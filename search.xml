<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[西瓜书训练营]]></title>
    <url>%2Fmachinelearning%2Fwatermeloncamp%2F</url>
    <content type="text"><![CDATA[\begin{equation} \begin{aligned} E_\hat{\omega}&=(y-X\hat{\omega})^T (y-X\hat{\omega}) \\ &=(y^T-\hat{\omega}^TX^T) (y-X\hat{\omega})\\ &=y^Ty - y^TX\hat{\omega} - \hat{\omega}^TX^Ty + \hat{\omega}^TX^TX\hat{\omega} \end{aligned} \end{equation} \begin{equation} \begin{aligned} \frac {\partial E_\hat{\omega}} {\partial \hat{\omega}} &=0-(y^TX)^T - X^Ty + [(X^TX)^T + X^TX]\hat{\omega}\\ &=-X^Ty - X^Ty + 2X^TX\hat{\omega}\\ &=2X^T(X\hat{\omega}-y) \end{aligned} \end{equation} 一般把向量定义为列向量 模型预测值逼近$y$的衍生物。 令$g(\cdot)​$为单调可微函数，则 y= g^{-1}(\omega x+b)称为广义线性模型，其中函数$g(\cdot)$称为联系函数。 若要使用线性模型进行分类任务是，则引入逻辑回归 输入：训练集 $D={ (xk, y_k) }{k=1}^m​$ ​ 梯度下降步长 $\alpha$，算法终止距离$\epsilon$ 过程： 在$(0,1)​$范围内随机初始化参数$\beta​$ repeat ​ for all $(x_k, y_k) \in D$ do ​ 根据当前参数和梯度公式计算当前样本的梯度$\Theta$； ​ 根据下降步长计算当前下降距离$\alpha \Theta$：若小于终止距离，则停止，否则下一步 ​ 更新梯度$\Theta = \Theta - \alpha \Theta$ ​ end for [1]. 向量微分]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo搭建个人博客]]></title>
    <url>%2FTools%2Fgithub_hexo%2F</url>
    <content type="text"><![CDATA[配置Hexo环境Ubuntu安装Node.js安装Node.js v11.x:123# Using Ubuntucurl -sL https://deb.nodesource.com/setup_11.x | sudo -E bash -sudo apt-get install -y nodejs 安装Hexo使用npm命令安装Hexo，输入：12345# 安装Hexo客户端npm install -g hexo-cli# 安装Hexo依赖npm install 利用 gulp 压缩代码1npm install gulp -g 生成ssh密钥文件：1ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot; 配置简历的PHP环境安装PHP sudo apt-get -y install php7.2 # 如果之前有其他版本PHP，在这边禁用掉 sudo a2dismod php5 sudo a2enmod php7.2 # 安装常用扩展 sudo apt-get -y install php7.2-fpm php7.2-mysql php7.2-curl php7.2-json php7.2-mbstring php7.2-xml php7.2-intl 安装composer 下载composercurl -sS https://getcomposer.org/installer | php 安装composer/usr/bin/php composer.phar --version 设置全局命令sudo mv composer.phar /usr/local/bin/composer 查看是否安装与设置成功composer --version 在仓库代码目录下执行composer install配置所需要的依赖 安装生成PDF文件的工具1.下载最新的包 http://wkhtmltopdf.org/downloads.html 2.安装依赖的组件：1sudo apt-get install libxfont2 xfonts-encodings xfonts-utils xfonts-base xfonts-75dpi 3.安装wkhtmltopdf： 1sudo dpkg -i wkhtmltox-\*.deb 4.测试： 1wkhtmltopdf https://baidu.com/ baidu.pdf 中文乱码问题——待解决 示例： ./bin/md2resume html --template swissen examples/source/sample.md examples/output/ ./bin/md2resume pdf --template swissen examples/source/sample.md examples/output/ Ubuntu下使用终端卸载软件包 # 查看已安装的程序 dpkg --list # 卸载程序和所有配置文件 sudo apt-get --purge remove &lt;packagename&gt; #只卸载程序，保留配置文件 sudo apt-get remove &lt;packagename&gt; [1].https://zhuanlan.zhihu.com/p/26625249 [2].https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html [3]. Install php7.2[4]. Install composer]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经纬度计算距离]]></title>
    <url>%2Fspark%2Fdistance_latitude_longitude%2F</url>
    <content type="text"><![CDATA[根据经纬度计算距离公式 S = 2\cdot R \cdot arcsin \sqrt{sin^2\frac{a}{2} + cos(lat1) \cdot cos(lat2) \cdot sin^2\frac{b}{2}}其中: (Lng1, lat1) 表示A点经纬度对应弧度，(lng2, lat2)表示B点经纬度对应弧度； $a=Lat1-Lat2$为两点纬度的弧度之差，$b=Lng1-Lng2$为两点经度的弧度之差； R为地球半径，$R=6378.137km$ 公式计算出来的结果单位为千米。若将半径改以米为单位，则计算的结果单位为米； 计算经度与谷歌地图的距离经度差不多，相差范围在0.2米以下 这种计算方式一般都是直线距离 一般地图上显示的左边顺序为：纬度在前（范围-90 ~ 90），经度在后（范围-180 ~ 180） 使用SQL计算距离的代码 1234SELECT *, 6378.138 *1000 * 2 * ASIN(SQRT(POW(SIN((lat1 * PI() / 180 - lat2 * PI() / 180) / 2), 2) + COS(lat1 * PI() / 180) * COS(lat2 * PI() / 180) * POW(SIN((lng1 * PI() / 180 - lng2 * PI() / 180) / 2), 2))) AS distanceFROM distanceORDER BY distance ASC 使用JAVA计算距离的代码 12345678910111213private static final double EARTH_RADIUS = 6378137;//赤道半径private static double rad(double d)&#123; return d * Math.PI / 180.0;&#125;public static double GetDistance(double lon1,double lat1,double lon2, double lat2) &#123; double radLat1 = rad(lat1); double radLat2 = rad(lat2); double a = radLat1 - radLat2; double b = rad(lon1) - rad(lon2); double s = 2 *Math.asin(Math.sqrt(Math.pow(Math.sin(a/2),2)+Math.cos(radLat1)*Math.cos(radLat2)*Math.pow(Math.sin(b/2),2))); s = s * EARTH_RADIUS; return s;//单位米&#125;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[相似性度量]]></title>
    <url>%2Fmachinelearning%2Fsimilarity%2F</url>
    <content type="text"><![CDATA[度量不同样本之间的相似性时，通常需要计算样本间的距离。而采用不同的计算方法将关系到算法的性能和准确性。一些常用的相似性度量方法有： 欧氏距离欧式距离是最易于理解的距离计算方法。 两个$n$维向量$a(x{11}, x{12}, x{13}, …, x{1n} )$与$b(x{21}, x{22}, x{23}, …, x{2n})$间的欧式距离： d_{12}=\sqrt {\sum_{k=1}^n (x_{1k} - x_{2k})^2}向量运算的形式： d_{12} = \sqrt{(a-b)(a-b)^T}例如常见在二维平面上两点$a(x_1, y_1)$与$b(x_2, y_2)$间的欧式距离： d_{12}=\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}曼哈顿距离想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离显然不是两点间的直线距离。实际驾驶距离就是这个“曼哈顿距离”，这也是曼哈顿距离名称的来源。曼哈顿距离也称为城市街区距离(City block distance)。 两个$n$维向量$a(x{11}, x{12}, x{13}, …, x{1n} )$与$b(x{21}, x{22}, x{23}, …, x{2n})​$间的曼哈顿距离： d_{12} = \sum_{k=1}^n|x_{1k}-x_{2k}|切比雪夫距离在国际象棋中，国王走一步能够移动到相邻的任意一个。那么国王从各自$(x_1, y_1)$走到格子$(x_2, y_2)$最少需要的步数为$max(|x_2-x_1|, |y_2-y_1|)$。有一种类似的距离度量方法叫切比雪夫距离。 两个$n$维向量$a(x{11}, x{12}, x{13}, …, x{1n} )$与$b(x{21}, x{22}, x{23}, …, x{2n})$间的切比雪夫距离： d_{12}=max_i(|x_{1i}-x_{2i}|)这个公式的另一种等价形式是 d_{12}=\lim_{k \rightarrow \infty }(\sum_{i=1}^n |x_{1i}-x_{2i}|^k)^\frac {1} {k}使用放缩法和夹逼法则证明两个公式等价 闵可夫斯基距离闵氏距离不是一种距离，而是一组距离的定义。 闵氏距离的定义： 两个$n​$维向量$a(x{11}, x{12}, x{13}, …, x{1n} )​$与$b(x{21}, x{22}, x{23}, …, x{2n})​$间的闵可夫斯基距离定义为： d_{12}=\sqrt[p]{\sum_{k=1}^n |x_{1k} - x_{2k}|^p}其中$p$是一个变参数：当$p=1$时，就是曼哈顿距离；当$p=2$时，就是欧氏距离；当$p\rightarrow \infty$ 时，就是切比雪夫距离。根据变参数的不同，闵氏距离可以表示一类的距离 闵氏距离的缺点 闵氏距离，包括曼哈顿距离、欧式距离和切比雪夫距离都存在明显的缺点。例如，二维样本(身高,体重)，其中身高范围是$150~190$，体重范围是50 ~ 60，有三个样本：$a(180,50)，b(190,50)，c(180,60)$。那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？因此用闵氏距离来衡量这些样本间的相似度很有问题。 简单说来，闵氏距离的缺点主要有两个：1. 将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。2. 没有考虑各个分量的分布(期望、方差等)可能是不同的。 标准化欧式距离标准化欧式距离是针对简单欧式距离的缺点而做的一种改进方案。标准欧式距离的思路：面对数据各维度分量的分布不一样，我们先将各个分量都”标准化”到均值、方差相等。那么均值和方差标准化到多少呢？假设样本集$X$的均值（mean）为$m$，标准差(standard deviation)为$s$，那么$X$的”标准化变量“表示为： 标准化变量的数学期望为0，方差为1。因此样本集的标准化过程(standardization)用公式描述就是： X^*=\frac{X-m}{s}标准化后的值 = （标准化前的值 - 分量的均值）/ 分量的标准差 经过简单的推导就可以得到两个$n$维向量$a(x{11}, x{12}, x{13}, …, x{1n} )$与$b(x{21}, x{22}, x{23}, …, x{2n})$间的标准化欧式距离的公式： d_{12}=\sqrt{\sum_{k=1}^n(\frac{x_{1k}-x_{2k}}{s_k})^2}如果将方差的倒数看成是一个权重，这个公式可以看成是一种加权欧式距离(Weighted Euclidean distance)。 马氏距离(Mahalanobis Distance)有$M$个样本向量$X_1, …, X_m$，协方差矩阵记为$S$，均值记为向量$\mu$的马氏距离表示为： D(X)=\sqrt{(X-\mu)^T S^{-1} (X-\mu)}而其中向量$X_i$与向量$X_j$之间的马氏距离定义为： D(X_i, X_j)=\sqrt{(X_i-X_j)^T S^{-1} (X_i-X_j)}若协方差矩阵是单位矩阵(各个样本向量之间独立同分布)，则公式为： D(X_i, X_j)=\sqrt{(X_i-X_j)^T (X_i-X_j)}即为欧式距离。 若协方差矩阵是对角矩阵，公式变成了标准化欧氏距离。 马氏距离的优缺点：量纲无关，排除变量之间的相关性的干扰 夹角余弦(Cosine)几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。 对于两个$n$维向量$a(x{11}, x{12}, x{13}, …, x{1n} )$与$b(x{21}, x{22}, x{23}, …, x{2n})$，可以使用类似于夹角余弦的概念来衡量它们间的相似程度： cos\theta = \frac{a \cdot b}{|a||b|}即： cos(\theta)=\frac{\sum_{k=1}^n x_{1k} x_{2k}}{\sqrt{\sum_{k=1}^n x_{1k}^2} \sqrt{\sum_{k=1}^n x_{2k}^2}}在二维空间中向量$A(x_1, y_1)$ 与向量$B(x_2, y_2)$的夹角余弦公式： cos\theta = \frac{x_1 x_2 + y_1 y_2}{\sqrt{x_1^2 + y_1^2} \sqrt{x_2^2 + y_2^2}}汉明距离(Hamming distance)定义：两个等长字符串$s_1$与$s_2$之间的汉明距离定义为将其中一个变为另外一个所需要做的最小替换次数。例如字符串”1111”与”1001”之间的汉明距离为2。 应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。 杰卡德距离 &amp; 杰卡德相似系数(Jaccard similarity coefficient) 杰卡德相似系数 两个集合A和B的交集元素在A, B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号$J(A,B)$表示： J(A,B)=\frac{|A\cap B|}{|A \cup B|}杰卡德相似系数是衡量两个集合的相似度一种指标 杰卡德距离 与杰卡德相似系数相反的概念是杰卡德距离(Jaccard distance)。杰卡德距离可用如下公式表示： J_{\sigma}(A, B) = 1-J(A,B) = \frac{|A \cup B|-|A \cap B|}{|A \cup B|}杰卡德距离用两个集合中不同元素占所有元素的比例来衡量两个 集合的区分的。 杰卡德相似系数与杰卡德距离的应用 可将杰卡德相似系数用在衡量样本的相似度上。 样本A与样本B是两个$n$维向量，而且所有维度的取值都是0或1。例如：$A(0111)$和$B(1011)$。我们将样本看成是一个集合，1表示集合包含该元素，0表示集合不包含该元素。 $p$：样本A与B都是1的维度的个数 $q$：样本A是1，样本B是0的维度的个数 $r$：样本A是0，样本B是1的维度的个数 $s$：样本A与B都是0的维度的个数 那么，样本A与B的杰卡德相似系数可表示为： 这里$p+q+r$可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。 而样本A与B的杰卡德距离表示为： J=\frac{p}{p+q+r} 相关系数(Correlation coefficient) &amp; 相关距离(Correlation distance) 相关系数的定义 \rho_{XY}=\frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}=\frac{E((X-EX)(Y-EY))}{\sqrt{D(X)}\sqrt{D(Y)}}相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是$[-1, 1]$。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。 相关距离的定义 D_{XY}=1- \rho_{XY} 信息熵(Information Entropy)信息熵是衡量分布的混乱程度或分散程度的一种度量。分布越分散(或者说分布越平均)，信息熵就越大。分布越有序（或者说分布越集中），信息熵就越小。 计算给定的样本集$X$的信息熵的公式： Entropy(X)=\sum_{i=1}^n p_i log_2\frac{1}{p_i}其中，$n$表示样本集$X$的分类数，$p_i$为$X$中第$i$类元素出现的概率。 信息熵越大表明样本集分类越分散，即随机变量的不确定性越大，信息熵越小则表明样本集分类越集中，此时随机变量的不确定性减小。当样本集$S$中$n$个分类出现的概率一样大时（都是$1/n$），信息熵取最大值$log_2n$。当$S$只有一个分类时，信息熵取最小值0。 其实熵拐了两个弯。熵意思是信息的混乱程度，越混乱，熵越大。分布越均匀，则是越混乱。 [1]. https://www.jianshu.com/p/991182ec5968 [2]. https://blog.csdn.net/wguangliang/article/details/49667109]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序、组合、阶乘]]></title>
    <url>%2Fmathematicas%2Farrangement_combination%2F</url>
    <content type="text"><![CDATA[排列的问题： 从n个不同元素中，拿出m个来进行排列，一共有多少种排列方法？ $A_n^m = {\frac {n!}{(n-m)!}}$ A就是Arrangement的缩写。 组合的问题： 从n个不同元素中，拿出m个来进行组合，一共有多少种组合方法？ $C_n^m =\frac{A_n^m}{m!}= {\frac {n!}{(n-m)!m!}}$ C就是Combination的缩写。 组合还有一种写法，见下图： $n,m∈N$； 当 $n&lt;m$ 时，$A = 0$ 并且 $C = 0$； $A_0^0 = C_0^0 = 0$ 排列分顺序，组合不分 阶乘： $n!$ 读作n的阶乘，并定义 $0! = 1$。 $C_n^0 = C_n^n = 1;n \neq 0$ 以上排列和组合的问题，是许多现实问题的抽象。]]></content>
      <categories>
        <category>基础数学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[【读书摘要】集成学习]]></title>
    <url>%2Fmachinelearning%2Fensemble_learning%2F</url>
    <content type="text"><![CDATA[An overview of ensemble methods in machine learning 为了得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立。尽管“独立”在现实任务中无法做到，但可以设法使得基学习器尽可能有较大的差异。事实上，个体学习器的“准确性”和“多样性”本身就存在冲突。一般的，准确性很高之后，要增加多样性就需要牺牲准确性。所以，如何产生并结合“好而不同”的个体学习器是集成学习研究的核心。 1. 生成个体学习器根据个体学习器的生成方式，目前的集成学习方法大致可以分为两大类：1. 个体学习器间存在强依赖关系、必须串行生成的序列化方法，其代表是Boosting；2. 个体学习器间不存在强依赖关系、可同时生成的并行化方法，其代表是Bagging和Random Forest。 1.1 串行化方法——BoostingBoosting是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制类似：先从初始化训练集出一个基学习器，再根据基学习器的表现对训练样本分布进行调整 ，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合. 从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。 1.2 并行化方法——Bagging和随机森林1.2.1 BaggingBootstrap，名字来自成语“pull up by your own bootstraps”，意思是依靠你自己的资源，称为自助法，是一种有放回的抽样方法，它是非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法。其核心思想和基本步骤如下： 采用重抽样技术从原始样本中抽取一定数量（自己给定）的样本，此过程允许重复抽样。 根据抽出的样本计算给定的统计量S 重复上述N次（一般大于1000），得到N个统计量S 计算上述N个统计量S的样本方差，得到统计量的方差 应该说Bootstrap是现代统计学较为流行的一种统计方法。在小样本时效果很好。通过方差的估计可以构造置信区间等，其运用范围得到进一步延伸。 还有一种抽样方法Jackknife，和Bootstrap功能类似，只是有一点细节不一样，即每次从样本中抽样时候只是去除几个样本（而不是抽样），就像小刀一样割去一部分。 Bagging，是由bootstrap aggregating缩写而成，是并行式集成学习方法最著名的代表。给定包含m个样本的数据集，采用有放回抽样的方式，得到含m个样本的采样集，初始训练集中有的样本在采样集中多次出现，有的则从未出现。如此可采样出T个包含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。这就是Bagging的基本流程。 从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。 1.2.2 随机森林随机森林（Random Forest，简称RF）是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，传统决策树在选择划分属性时是在当前节点的属性集合（假定有d个属性）中选择一个最优属性；而在RF中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性的引入程度：若令$k=d$，则基决策树的构建与传统决策树相同；若令$k=1$，则是随机选择一个属性用于划分；一般情况下，推荐值$k=log_2d$。 2. 结合策略假定集成包含T个基学习器$\left { h_1, h_2, h_3, …, h_T \right }$，其中$h_i$在示例x上的输出为$h_i(x)$。 2.1 平均法 简单平均法H(x)=\frac{1} {T} \sum_{i=1}^T h_i(x) 加权平均法H(x) = \sum_{i=1}^T \omega_i h_i(x)其中$\omegai$是个体学习器$h_i$的权重，通常要求$\omega_i\geq 0, \sum{i=1}^T \omega_i =1$. 一般而言，在个体学习器性能相差较大时宜使用加权平均法，而在个体学习器性能相近时宜使用简单平均法。加权平均法的权重一般是从训练数据中学习而得，比如估计出个体学习器的误差，然后令权重大小与误差大小成反比，现实任务中的训练样本通常不充分或存在噪声，这将使得学出的权重不完全可靠。 2.2 投票法对分类任务来说，学习器将从类别标记集合${c_1, c_2, c_3, …, c_N}$中预测出一个标记，最常见的结合策略是使用投票法。将h_i在样本x上的预测输出表示为一个N维向量$(h_i^1(x); h_i^2(x); …; h_i^N(x))$，其中$h_i^j(x)$是$h_i$在类别标记$c_j$上的输出。 绝对多数投票法H(x)=\left\{\begin{matrix} c_j, & if \sum_{i=1}^Th_i^j(x)>0.5\sum_{k=1}^N \sum_{i=1}^T h_i^k(x); \\ reject, & otherwise. \end{matrix}\right.即若某标记得票过半数，则预测为该标记；否则拒绝预测。 相对多数投票法H(x)=c_{arg_jmax\sum_{i=1}^Th_i^j(x)} 加权投票法H(x)=c_{arg_jmax\sum_{i=1}^T \omega_i h_i^j(x)}$\omegai$是$h_i$的权重，通常要求$\omega_i\geq 0, \sum{i=1}^T \omega_i =1$. 在现实任务中，不同类型个体学习器可能产生不同类型的$h_i^j(x)$值，而不同类型的$h_i^j(x)$值不能混用。 2.3 学习法当训练数据很多时，可以通过另一个学习器来进行结合，从而形成一种更强大的结合策略。Stacking是学习法的典型代表。Stacking本身是一种注明的集成学习方法，且有不少集成学习算法可视为其变体或特例。此处，将其看作一种特殊的结合策略。Stacking先从初始数据集训练出初级学习器，然后 “生成”一个新数据集用用训练刺激学习器。在这个心数据集中，初级学习器的输出被当做样例输入特征，而初始样本的标记仍被当做样例标记。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[42，一个神奇的数字]]></title>
    <url>%2Funcategorized%2F42%2F</url>
    <content type="text"><![CDATA[无处不在…不知道你在学习视频或者钻研大佬写的代码的时候是否注意到一个普遍的现象：很多时候大佬们都喜欢用数字“42”作为随机数的种子。例如下面这个DataCamp的课程： “42”不仅受到程序员的欢迎，而且更加生气的是当你在Google中输入： the answer to life the universe and everything 的时候，Google计算器会告诉你答案——42。 宇宙的终极问题……为什么42竟然会成为宇宙和时间万物的答案？这得从英国科幻作家道格拉斯·亚当斯所写的经典科幻小说《银河系漫游指南》讲起。在故事中，一个具有高度智慧的跨维度生物种族为了找出一个能够回答终极问题的简单答案，特别造了一台超级电脑——“深思”（Deep Thought）来进行计算。“深思”花了750万年来计算和验证，最后得出了“42”这个答案。 当被要求提供所谓的终极问题时，“深思”说它没办法，但是它可以设计出另外一台更强大的电脑（也就是地球这个生体电脑）来做这工作。于是当初问这些问题的种族就开始了漫长无尽的等待，让这个超级生体电脑去运行程式来找出终极问题。经过了800万年，就在结果要出来的五分钟前，地球却因为挡在预定兴建的星际间高速公路的路线，被渥罡人给毁灭，电脑没有给出最后的结果。作者并没有确切地说出“终极问题”到底是什么，不过有许多读者提供了不少的理论和意见给作者作为参考。 从此，42也就成为一个“梗”在科学家、程序员以及极客之间流传了开来。在知识引擎WolframAlpha平台中，42同样也被作为了宇宙万物的答案： 在Siri中问“What’s the meaning of life?”，Siri也会回答42。在英雄联盟中召唤峡谷地图中使用汉默丁格能偶尔听到他说到“42，一个神秘的数字，某些东西与它有关~”甚至有人说《佛说四十二章经》中的42也与之有关，emmmm…… 了解了这些，以后在编程中把“宇宙的终极答案”作为随机数的种子，是不是突然有一种很酷的感觉？]]></content>
  </entry>
  <entry>
    <title><![CDATA[SQL中的变量绑定]]></title>
    <url>%2Fspark%2Fsql%2F</url>
    <content type="text"><![CDATA[在Spark-sql的使用过程中可能会涉及到变量的设定 123456789101112131415161718192021222324252627set hivevar:msg=&#123;"message":"2015/12/08 09:14:4","client": "10.108.24.253","server": "passport.suning.com","request": "POST /ids/needVerifyCode HTTP/1.1","server": "passport.sing.co","version":"1","timestamp":"2015-12-08T01:14:43.273Z","type":"B2C","center":"JSZC","system":"WAF","clientip":"192.168.61.4","host":"wafprdweb03","path":"/usr/local/logs/waf.error.log","redis":"192.168.24.46"&#125;;select a.* lateral view json_tuple('$&#123;hivevar:msg&#125;','server','host', 'version') a as server, host, version; -- 动态查询后绑定变量（该执行不生效）SET hivevar:gridSize = (SELECT max(grid_size) FROM cfg_coverage_threshold);-- 查询环境中已经存在的变量SELECT '$&#123;hivevar:spark.app.id&#125;';SELECT '$&#123;hivevar:spark.sql.thriftServer.limitCollectNumber&#125;';-- 查询环境中所有变量SET;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Antlr4应用初探]]></title>
    <url>%2Funcategorized%2Fantlr4%2F</url>
    <content type="text"><![CDATA[1. Antlr4是什么？当我们实现一种语言时，我们需要构建读取句子（sentence）的应用，并对输入中的元素做出反应。如果应用计算或执行句子，我们就叫它解释器（interpreter），包括计算器、配置文件读取器、Python解释器都属于解释器。如果我们将句子转换成另一种语言，我们就叫它翻译器（translator），像Java到C#的翻译器和编译器都属于翻译器。不管事解释器还是翻译器，应用首先都要识别出所有有效的句子、词组、字词组等，识别语言的程序就叫解析器（parser）或语法分析器（syntax analyzer）。我们学习的终点就是如何实现自己的解析器，去解析我们的目标语言，像DSL语言、配置文件、自定义SQL等待。 1.1 元编程手动编写解析器是非常繁琐的，所以我们有了ANTLR。只需要编写ANTLR的语法文件，描述我们要解析的语言的语法，之后ANTLR就会自动生成能解析这种语言的解析器。也就是说，ANTLR是一种能写出程序的程序。在学习LISP或Ruby的宏时，我们经常能接触到元编程的概念。而用来声明我们语言的ANTLR语言的语法，就是元语言（meta language）。 1.2 解析过程为了简单起见，我们将解析分为两个阶段，对应我们的大脑读取文字时的过程。当我们读到一个句子时，在第一阶段，大脑会下意识地将字符组成单词，然后像查词典一样识别出它的意思。在第二阶段，大脑会根据已识别的单词去识别句子的结构。第一阶段的过程叫词法分析（lexical analysis），对应的分析程序叫lexer，负责将符号（token）分组成符号类（token class or token type）。而第二阶段就是真正的paser，默认ANTLR会构建出一颗分析树（parse tree）或叫语法树（syntax tree）。如下图，就是简单的赋值表达式的解析过程： 语法树的叶子是输入token，而上级结点是包含其孩子结点的词组名（phase），线性的句子其实是语法树的序列化。最终生成语法树的好处是：树形结构易于遍历和处理，并且易被程序员理解，方便了应用代码做进一步处理。 多种解释或翻译的应用代码都可以重用一个解析器。但ATRLR也支持像传统解析器生成器那样，将应用处理代码嵌入到语法中。 对于因为计算依赖而需要多趟处理的翻译器来说，语法树非常有用！我们不用多次调用解析器去解析，只需要高效地遍历语法树多次。 2. ANTLR在IDE中的应用2.1 安装IDE插件这里使用的是Intellij IDEA，所以在Plugins中搜“ANTLR v4 grammar plugin”插件，重启IDEA即可使用。如果想在IDE外使用，需要下载ANTLR包，是JAVA写成的，后面在IDEA中的各种操作都可以手动执行命令来完成。 2.2 动手实现解析器2.2.1 编写.g4文件创建一个文件，后缀名是g4，只有这样在文件上点右键才能看到ANTLR插件的菜单。1234567891011121314151617181920212223/** * This grammar is called ArrayInit and must match the filename: ArrayInit.g4 */grammar ArrayInit;​/* ============================================= *//* Parser rules start with lowercase letters. *//* ============================================= */​/** A rule called init that matches comma-separated values between &#123;...&#125;. */init : &apos;&#123;&apos; value (&apos;,&apos; value)* &apos;&#125;&apos; ; // must match at least one value​/** A value can be either a nested array/struct or a simple integer (INT) */value : init | INT ;​/* ============================================= *//* Lexer rules start with uppercase letters. *//* ============================================= */​INT : [0-9]+ ; // Define token INT as one or more digitsWS : [ \t\r\n]+ -&gt; skip ; // Define whitespace rule, toss it out 2.2.2 自动生成代码在.g4文件上右键就能看到ANTLR插件的两个菜单，分别用来配置ANTLR生成工具的参数（在命令行中都有对应）和触发生成文件。首先选配置菜单，将目录选择到main/java或者test/java。注意：ANTLR会自动根据Pacakage/namespace的配置，生成包的文件夹，不用预先创建。之后就点击生成菜单，于是就在我们配置的目录下自动生成如图所示代码。 2.3 构建应用代码有了生成好的解析器，我们就可以在此基础上构建应用了。 2.3.1 添加运行依赖在开始编写应用代码之前，我们要引入ANTLR运行依赖。因为我们的解析器其实只是一堆回调hook，真正的通用解析流程实现是在ANTLR runtime包中。所以，以sbt为例ANTLR V4的依赖是： 12// https://mvnrepository.com/artifact/org.antlr/antlr4-runtimelibraryDependencies += "org.antlr" % "antlr4-runtime" % "4.7.2" 2.3.2 应用代码我们实现一个Listener完成翻译工作。然后在main()中构建起词法分析器和解析器，已经连接他们的数据流和语法树。 12345678910111213141516171819202122232425262728import antlr4.gen.ArrayInitLexer;import antlr4.gen.ArrayInitParser;import org.antlr.v4.runtime.ANTLRInputStream;import org.antlr.v4.runtime.CommonTokenStream;import org.antlr.v4.runtime.tree.ParseTree;import org.antlr.v4.runtime.tree.ParseTreeWalker;public class Main &#123; public static void main(String[] args) &#123; // String sentence = "&#123;1, &#123;2, 3&#125;, 4&#125;"; String sentence = "&#123;99, 3, 451&#125;"; // 1.Lexical analysis 词法分析 ArrayInitLexer lexer = new ArrayInitLexer(new ANTLRInputStream(sentence)); CommonTokenStream tokens = new CommonTokenStream(lexer); // 2.Syntax analysis 语法分析 ArrayInitParser parser = new ArrayInitParser(tokens); ParseTree tree = parser.init(); // 3.Application based on Syntax Tree ParseTreeWalker walker = new ParseTreeWalker(); walker.walk(new ShortToUnicodeString(), tree); System.out.println(); &#125;&#125; ShortToUnicodeString类 12345678910111213141516171819202122232425import antlr4.gen.ArrayInitBaseListener;import antlr4.gen.ArrayInitParser;import com.sun.istack.internal.NotNull;​public class ShortToUnicodeString extends ArrayInitBaseListener &#123;​ @Override public void enterInit(@NotNull ArrayInitParser.InitContext ctx) &#123; System.out.print('"'); &#125;​ @Override public void exitInit(@NotNull ArrayInitParser.InitContext ctx) &#123; System.out.print('"'); &#125;​ @Override public void enterValue(@NotNull ArrayInitParser.ValueContext ctx) &#123; if (ctx.INT() == null) &#123; System.out.print(ctx.INT()); &#125; else &#123; System.out.printf("\\u%04x", Integer.valueOf(ctx.INT().getText())); &#125; &#125;&#125; [1]. Antlr v4入门教程和实例 [2]. Spark SQL 中ANTLR4的应用 [3]. 如何用 ANTLR 4 实现自己的脚本语言？]]></content>
  </entry>
  <entry>
    <title><![CDATA[68-95-99.7法则和均方根误差]]></title>
    <url>%2Fdatascience%2Fnormal_distribution%2F</url>
    <content type="text"><![CDATA[回归问题的典型指标是均方根误差（RMSE）。均方根误差测量的是系统预测误差的标准差。例如，RMSE 等于 50000，意味着，68% 的系统预测值位于实际值的 50000 美元以内，95% 的预测值位于实际值的 100000 美元以内（一个特征通常都符合高斯分布，即满足 “68-95-99.7”规则：大约68%的值落在 1σ 内，95% 的值落在 2σ 内，99.7%的值落在 3σ 内，这里的 σ 等于50000） Pr(\mu - \sigma \leqslant X \leqslant \mu + \sigma) \approx 0.6827 \\ Pr(\mu - 2\sigma \leqslant X \leqslant \mu + 2\sigma) \approx 0.9545 \\ Pr(\mu - 3\sigma \leqslant X \leqslant \mu + 3\sigma) \approx 0.9973In statistics, the 68–95–99.7 rule, also known as the empirical rule, is a shorthand used to remember the percentage of values that lie within a band around the mean in a normal distribution with a width of two, four and six standard deviations, respectively; more accurately, 68.27%, 95.45% and 99.73% of the values lie within one, two and three standard deviations of the mean, respectively. 在实际应用上，常考虑一组数据具有近似于正态分布的概率分布。若其假设正确，则约 68% 数值分布在距离平均值有 1 个标准差之内的范围，约 95% 数值分布在距离平均值有 2 个标准差之内的范围，以及约 99.7% 数值分布在距离平均值有 3 个标准差之内的范围。称为 “68-95-99.7法则”或”经验法则”. 68–95–99.7 rule]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[快速排序算法]]></title>
    <url>%2Falgorithm%2Fquick_sort%2F</url>
    <content type="text"><![CDATA[参考常用排序算法总结（性能+代码） 快速排序算法是基于“二分”的思想，其排序过程可由下图体现 快速排序的每一轮就是将这一轮的基准数归位，直到所有的数都归位为止，排序结束（类似于冒泡算法）。 partition是返回一个基准值的index，index左边都小于该index的数，右边都大于该index的数。 快速排序之所以比较快，因为相比冒泡排序，每次交换是跳跃式的。每次排序的时候设置一个基准点，将小于等于基准点的数全部放到基准点的左边，将大于等于基准点的数全部放到基准点的右边。这样在每次交换的时候就不会像冒泡排序一样每次只能在相邻的数之间进行交换，交换的距离就大的多了。因此总的比较和交换次数就少了，速度自然就提高了。当然在最坏的情况下，仍可能是相邻的两个数进行了交换。因此快速排序的最差时间复杂度和冒泡排序是一样的都是​，它的平均时间复杂度为​。 快速排序的优化 三者取中法 由于每次选择基准值都选择最后一个，这就会产生一个问题，那就是可能会造成每次都需要移动，这样会使算法性能很差，趋向于​，所以我们要找出中间位置的值。我们希望基准值能够更接近中间位置的值，所以这里可以每次使用待排序的数列部分的头、尾、中间数，在三个数中取中间大小的那个数作为基准值，然后进行快速排序，这样能够对一些情况进行优化。 根据规模大小改变算法 由于快速排序在数据量较小的情况下，排序性能并没有其他算法好，所以我们可以在待排序的数列分区小于某个值后，采用其他算法进行排序，而不是继续使用快速排序，这时也能得到一定的性能提升。一般这个值可以为5~25，在一些编程语言中使用10或15作为这个量。 其他分区方案考虑 有时，我们选择的基准数在数列中可能存在多个，这时我们可以考虑改变分区的方案，那就是分三个区间，除了小于基准数的区间、大于基准数的区间，我们还可以交换出一个等于基准数的区间，这样我们在之后每次进行递归时，就只递归小于和大于两个部分的区间，对于等于基准数的区间就不用再考虑了。 并行处理 由于快速排序对数组中每一小段范围进行排序，对其他段并没有影响，所以可以采用现在计算机的多线程并行处理来提高效率，这并不算是对算法的优化，只能说是一种对于数量稍微多一些的数据使用快速排序时的一个高效解决方案。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package mophei.algorithm.ch03;public class Quicksort2 &#123; public int partitionByLeft(int[] A, int begin, int end) &#123; int left = begin; int right = end; int pivot = begin; int pivotnumber = A[pivot]; while (left != right) &#123; while (A[right] &gt; pivotnumber &amp;&amp; left &lt; right) &#123; right--; &#125; // 在右侧找到第一个不大于基准值的元素 while (A[left] &lt;= pivotnumber &amp;&amp; left &lt; right) &#123; left++; &#125; // 在左侧找到第一个大于基准值的元素 swap(A, left, right); // 将找到的小于等于基准值的元素换到左边，大于基准值的元素换到右边 &#125; swap(A, left, pivot); return left; &#125; public int partitionByRight(int[] A, int begin, int end) &#123; int left = begin; int right = end; int pivot = end; int pivotNumber = A[pivot]; while (left != right) &#123; /** * 从左边先开始遍历，决定相遇时的值是大于基准值 */ while (A[left] &lt; pivotNumber &amp;&amp; left &lt; right) &#123; left++; &#125; while (A[right] &gt;= pivotNumber &amp;&amp; left &lt; right) &#123; right--; &#125; swap(A, left, right); &#125; swap(A, right, pivot); return left; &#125; public int partition(int[] A, int begin, int end) &#123; int pivot = end; int pivotNumber = A[pivot]; for (int i = end; i &gt; -1; i--) &#123; if (A[i] &gt; pivotNumber) &#123; pivot--; swap(A, i, pivot); &#125; &#125; swap(A, end, pivot); return pivot; &#125; private void swap(int[] A, int left, int right) &#123; int temp = A[left]; A[left] = A[right]; A[right] = temp; &#125; public void sort(int[] A, int begin, int end) &#123; if (begin &lt; end) &#123; int q; q = partition(A, begin, end); sort(A, begin, q - 1); sort(A, q + 1, end); &#125; &#125; public static void main(String[] args) &#123;// int[] array = &#123;8, 7, 1, 6, 5, 4, 3, 2&#125;; int[] array = &#123;47, 29, 71, 99, 78, 19, 24, 47&#125;; Quicksort2 s = new Quicksort2(); s.sort(array, 0, 7); for (int i = 0; i &lt; array.length; i++) &#123; System.out.println("output " + array[i]); &#125; &#125;&#125;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分类场景下的类别不平衡问题]]></title>
    <url>%2Fmachinelearning%2Fclassfication%2F</url>
    <content type="text"><![CDATA[https://zhuanlan.zhihu.com/p/32940093 https://blog.csdn.net/heyongluoyao8/article/details/49408131 机器学习中常常会遇到数据的类别不平衡（class imbalance），有时也叫数据类别偏斜（class skew）。以常见的二分类问题为例，我们希望预测信用卡用户是否存在欺诈行为。但在历史数据中，存在欺诈行为的比例可能很低（比如0.1%）。在这种情况下，学习出好的分类器是很难，而且在得到的结论往往也很具有迷惑性。比如在诈骗行为预测中，如果我们的分类器总是预测一个用户不存在诈骗行为，即预测为反例，那么我们依然有高达99.9%的预测准确率。然而这种结论是没有意义的。那么，在类别不平衡的情况下如何有效的评估分类器呢？这就是我们首先需要研究的问题。 1. 类别不平衡下的评估问题对于平衡的数据，我们一般都用准确率（accuracy），也就是1-误分率作为一般的评估标准。这种标准的默认假设前提是：“数据是平衡的，正例与反例的重要性一样，二分类的阈值是0.5”。在这种情况下，用准确率来对分类器进行评估是合理的。 当类别不平衡时，准确率就非常具有迷惑性，而且意义不大。以下给出几种主流的评估方法： ROC曲线（receiver operating characteristic curve, 接收者操作特征曲线）是一种常见的替代方法，计算ROC曲线下的面积是一种主流方法 Precision-recall curve和ROC有相似的地方，但定义不同，计算此曲线下的面积也是一种方法 Precision@n 是另一种方法，特指将分类阈值设定到恰好n个正例时分类器的precision Average precision 也叫做平均精度，主要描述了precision的一般表现，在异常检测中有时候会用 直接使用Precision也是一种想法，但此时的假设是分类器的阈值是0.5，因此意义不大 至于哪种方法更好，一般来看我们在极端类别不平衡中更在意“少数的类别”（即需要预测小概率事件的出现），因此ROC不像precision-recall curve那样更具有吸引力。在这种情况下，precision-recall curve不失为一种好的评估标准，相关的分析可以参考[2]。还有一种做法是仅分析ROC曲线左侧的一小部分，从这个角度看和precision-recall cureve有很高的相似性。 同理，因为我们更在意罕见的正例，因此precision尤为重要，因此average precision（macro）也是常见的评估标准。此处需要提醒两点： 没有特殊情况，不要用准确率（accuracy），一般都没有什么帮助 如果使用precision，请注意调整分类阈值，precision@n更有意义 更为一般的分类评估标准，简单的科普可以参考：如何解释召回率与准确率? 2. 解决类别不平衡中的”奇淫技巧”有什么？在[1]中介绍了很多比较负载的技巧。结合我的了解举几个简单的例子： 对数据进行采用的过程中通过相似性同时生成并插样“少数类别数据”，叫做SMOTE算法 对数据先进行聚类，再将大的簇进行随机欠采样或者小的簇进行数据生成 把监督学习变为无监督学习，舍弃标签把问题转为一个无监督问题，如异常检测 先对多数类别进行随机的欠采样，并结合boosting算法进行集成学习 科普文[3]中介绍了一部分上面提到的算法，可以进行参考。 3. 简单通用的算法有哪些？除了第二节中提到的一些看起来略微复杂的算法，最贱的算法无外乎三种，在大部分教材中都有涉猎[4]： 对较多的那个类别进行欠采样(under-sampling)，舍弃一部分数据，使其与较少类别的数据相当 对比较少的类别进行过采样(over-sampling)，重复使用一部分数据，使其与较多类别的数据相当 阈值调整(threshold moving)，将原本默认为0.5的阈值调整到较少类别/(较少类别+较多类别)即可 当然，第一种和第二种方法都会明显的改变数据分布，我们的训练数据假设不再是真实数据的无偏表述。在第一种方法中，我们浪费了很多数据；而第二种方法中无中生有或重复使用了数据，会导致过拟合的发生。 在欠采样的逻辑中往往会结合集成学习来有效的使用数据：假设正例数据n个，反例数据m个。我们可以通过欠采样，随机无重复的生成​个反例子集，并将每个子集都与相同正例数据合并生成​个新的训练样本。我们在​个训练样本上分别训练一个分类器，最终将​个分类器的结果结合起来，比如求平均值或者简单投票。这就是一个简单的思路，也就是Easy Ensemble[5] 然而，这样的过程是需要花时间处理数据和编程的，难度比较大。所以推荐两个简单易行且效果中上 的做法： 简单的调整阈值，不对数据进行任何处理。此处特指将分类阈值从0.5调整到正例比例 使用现有的集成学习分类，如随机森林或者xgboost，并调整分类阈值 提出这样建议的原因有很多。首先，简单的阈值调整从经验上看往往比过采样和欠采样有效[6]。其次，如果你对统计学知识掌握有限，而且编程能力一般，在集成过程中更容易出错，还不如使用现有的集成学习并调整分类阈值。 4. 一个简单但有效的方案经过上文的分析，我任务一个比较靠谱的解决方案是： 不对数据进行过采样和欠采样，但使用现有的集成学习模型，如随机森林 输出随机森林的预测概率，调整阈值得到最终结果 选择合适的评估标准，如precision@n 这种方法难度很低，也规避了不少容易出错的地方。我们使用了集成学习降低过拟合风险，使用阈值调整规避和采样问题，同时选择合适的评估手段以防止偏见。而且这些都是现成的模型，5-10行的python代码就可以实现。有兴趣的朋友可以在这个基础上进行更多探索，而把这个结果作为一个基准(baseline)。 当然，更多复杂的操作是可以的，比如[7]就在欠采样集成后使用了逻辑回归作为集成分类器来学习不同子训练集的权重，并通过L1正则自动舍弃掉一部分基学习器。当然，我很怀疑这种结果是否比得上简单的处理模式。 [1] He, H. and Garcia, E.A., 2009. Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9), pp.1263-1284. [2] Davis, J. and Goadrich, M., 2006, June. The relationship between Precision-Recall and ROC curves. In Proceedings of the 23rd international conference on Machine learning (pp. 233-240). ACM. [3] How to handle Imbalanced Classification Problems in machine learning? [4] 周志华。机器学习，清华大学出版社，3.7，2016。 [5] Liu, T.Y., 2009, August. Easyensemble and feature selection for imbalance data sets. In Bioinformatics, Systems Biology and Intelligent Computing, 2009. IJCBS’09. International Joint Conference on (pp. 517-520). IEEE. [6] Han, J., Pei, J. and Kamber, M., 2011. Data mining: concepts and techniques. Elsevier. [7] Micenková, B., McWilliams, B. and Assent, I., 2015. Learning representations for outlier detection on a budget. arXiv preprint arXiv:1507.08104.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘中常见的「异常检测」算法]]></title>
    <url>%2Fmachinelearning%2Foutlier_detection%2F</url>
    <content type="text"><![CDATA[转自知乎问答数据挖掘中常见的「异常检测」算法有哪些？ 一般情况下，可以把异常检测看成是数据不平衡下的分类问题。因此，如果数据条件允许，优先使用有监督的异常检测[6]。实验结果[4]发现直接用XGBoost进行有监督异常检测往往也能得到不错的结果，没有思路时不妨一试。 而在仅有少量便签的情况下，也可以采用半进度异常检测模型。比如把无监督学习作为一种特征抽取方式来辅助监督学习[4, 8]，和stacking比较类似。这种方法也可以理解成通过无监督的特征工程对数据进行预处理，喂给有监督的分类模型。 但在现实情况中，异常检测问题往往是没有便签的，训练数据中并未标出哪些是异常点，因此必须使用无监督学习。从使用角度出发，文章重点关注无监督学习。 本文结果如下：1. 介绍常见的无监督异常算法及实现；2. 对比多种算法的检测能力； 3. 对比多种算法的运算开销； 4. 总结并归纳如何处理异常检测问题； 5. 代码重现步骤。 1. 无监督异常检测如果归类的话，无监督异常检测模型可以大致分为： 统计与概率模型（statistical and probabilistic models）：主要是对数据的分布做出假设，并找出假设下所定义的“异常”，因此往往会使用极值分布或者假设检验。比如对最简单的以为数据假设搞死分，然后将距离简直特定范围以外的数据当作异常点（$3-6\sigma$检测）。而推广到高维后，可以假设每个维度各自独立，并将各个维度上的异常度相加 （具体如何理解呢，是否有具体例子） 。如果考虑特征间的相关性，也可以用马氏距离（Mahalanobis distance）来衡量数据的异常度[12]。不难看出，这类方法最大的好处就是速度一般比较快，但因为存在比较强的“假设”，效果不一定很好。 线性模型（Linear Models）：假设数据在低维空间上有嵌入（什么意思？），那么无法、或者在低维空间投射后表现不好的数据可以认为是离群点。举个简单的例子，PCA可以用于做异常检测[10]，一种方法就是找到​个向量特征（eigenvector），并计算每个样本在经过这​个特征向量投射后的重建误差（reconstruction error），而正常点的重建误差应该小于异常点。同理，也可以计算每个样本到这​个特征向量所构成的超空间的加权欧氏距离（特征值越小权重越大）。在相似的思路下，我们可以直接对协方差居中进行分析，并把样本的马氏距离（在考虑特征间关系时样本到分布中的距离）作为样本的异常度，而这种方法也可以被理解为一种软性（Soft PCA）[6]。同时，另一种经典算法One-class SVM[3]也一般被归类为线性模型。 基于相似度衡量的模型（Proximity bases models）：异常点因为和正常点的分布不同，因此相似度较低，由此衍生了一系列算法通过相似度来识别异常点。比如最简单的​近邻就可以做异常检测，一个样本和它第​个近邻的距离就可以被当做是异常值，显然异常点的​近邻距离更大。同理，基于密度分析如LOF[1]、LOCI和LoOP主要是通过局部的数据密度来检测异常。显然，异常点所在空间的数据点少、密度低。相似的是，Isolation Forest[2]通过划分超平面来计算”孤立“一个样本所需的超平面数量（可以想象成在想吃蛋糕上的樱桃所需的最少刀数）。在密度低的空间里（异常点所在空间中），孤立一个样本所需要的划分次数更少。另一种相似的算法ABOD[7]是计算每个样本与所有其他样本对所形成的夹角的方差，异常点因为远离正常点，因此方差变化小。话句话说，大部分异常检测算法都可以被认为是一种估计相似度，无论是通过密度、距离、夹角或是划分超平面。通过聚类也可以被理解为一种相似度度量，比较常见不在赘述。 集成异常检测与模型融合：在无监督学习是，提高模型的鲁棒性很重要，因此集成学习就大有用武之地。比如上面提到的Isolation Forest，就是基于构建多棵决策树实现的。最早的集成检测框架feature bagging[19]与分类问题中的随机森林（random forest）很像，先将训练数据随机划分（每次选取所有样本的d/2~d个特征，d代表特征数），得到多个子训练集，再在每个训练集上训练一个独立的模型（默认为LOF）并最终合并所有的模型结果（如通过平均）。值得注意的是，因为没有标签，异常检测往往是通过bagging和feature Bagging比较多，而boosting比较少见。boosting情况下的异常检测，一般需要生成为标签，可参考[13,14]。集成异常检测是一个新兴但很有趣的领域，综述文章可以参考[16,17,18]。 特定领域上的异常检测：比如图像异常检测[21]，顺序及流数据异常检测（时间序列异常检测）[22]，以及高维度空间上的异常检测[23]，比如前文提到的Isolation Forest就很适合高维数据上的异常检测。 不难看书，上文提到的划分标准其实是互相交织的。比如k-近邻可以看做是概率模型非参数化后的一种变形，而通过马氏距离计算异常度虽然是线性模型，但也对分布有假设（高斯分布）。Isolation Forest虽然是集成学习，但其实和分析数据的密度有关，并且适合高维数据上的异常检测。在这种基础上，多种算法其实是你中有我，我中由你，相似的理念都可以被推广和应用，比如计算重建误差不仅可以用PCA，也可以用神经网络中的auto-encoder。另一种划分异常检测模型的标准可以理解为局部算法（local）和全局算法（global），这种划分方法时考虑到异常点的特性。想要了解更多异常检测还是推荐看经典教科书Outlier Analysis[6]，或者综述文章[15]。 虽然一直有新的算法被提出，但因为需要采用无监督学习，且我们队数据分布的有限了解，模型选择往往还是采用试错法，因此快速迭代地尝试大量的算法就是一个必经之路。我们会对比多种算法的预测能力、运算开销及模型特点。如无特别说明，本文找那个的图片、代码均来自于开源Python异常检测工具库Pyod。文中实验所使用的17个数据集均来自于（ODDS-Outlier Detection DataSets）。 本文中所对比的算法（详见Pyod介绍）： PCA [10] MCD: Minimum Covariance Determinant [11, 12] OCSVM: One-class Support Vector Machines [3] LOF: Local Outlier Factor [1] kNN: k Nearest Nerghbors [19, 20] HBOS: Histogram-based Outlier Score [5] FastABOD: Fast Angle-Based Outlier Detection using approximation [7] Isolation Forest [2] Feature Bagging [9] 先来看下这九种算法在人工生成的数据上的表现（正常点由高斯分布生成，异常点由均匀生成）。在这种简单数据上，我们发现大部分算法的表现都不错，值得关注的有几点： PCA（图7）和MCD（图9）的决策边界（decision boundary）非常相似，我们前文提到过，协方差矩阵可以看做是PCA的软性版本。其实线性模型的决策边界都有共性，图2中的one-class SVM he PCA及MCD的决策边界长得也差不多。而且你回发现这些线性模型的形成可视化是一圈一圈向外降低的，这恰恰是因为对数据分布假设的原因。 HBOs（图5）的分类边界像是多个长方形组合在一起。这和它的原理有关，HBOS全称Histogram-based outlier score，它假设每个维度独立并在每个维度上划分n个区间，每个区间所对应异常值取决于密度。密度越高，异常值越低，因此也可以看成是一种假设维度独立的密度检测。一定程度上，HBOS和Isolation Forest有一定的相似性，看Isolation Forest（图4）很像是一种加了随机性的HBOS（有相似处但不是）。 ABOD（图6）中的决策边界非常复杂，显然存在一定程度上的过拟合，效果也不是太好，具体原因我们稍后再分析。 通过这个toy example我们想给大家看一下算法间的联系。 2. 模型检测效果我们采用ROC和Precision@Rank n (prn)作为衡量标准。ROC大家很熟悉了（要做到不查资料说出其概念），而后者指的是在假设有n个异常点时的经度（precision），和推荐系统中的precision @ k 是一样的。异常检测因为数据不平衡，一般不适合用准确度（误分率）来进行评估。 在下面的结果图表中，最优（次优）模型均用黑体加粗，最差用红色字体标出。 对以上结果进行初步分析： 总体来看，没有任何模型表现持续最好。Isolation Forest和KNN的表现非常稳定，分别在ROC和PRN上有比较优秀的表现。 KNN等模型基于距离度量，因此受到数据维度的影响比较大，当维度比较低时表现很好。如果异常特征隐藏在少数维度上时，KNN和LOF类的效果就不会太好，此时该选择Isolation Forest。 部分模型较不稳定，受到数据裱花的影响较大，如HBOS在8个数据上表现优异，但也在3个数据上表现最差。 简单模型的表现不一定差，比如KNN和MCD的原理都不负责，但表现均不错。 ABOD综合看效果最差，应该避免。这个实验中所用的ABOD版本是FastABOD并用10个近邻进行近似，可能精准版的ABOD效果会好一些，但运算复杂度会大幅度上升（​）。 3. 模型运算开销 结合分析模型的运算开销，我们可以得到一些结论： Feature Bagging的运算开销很高，这个是可以理解的，因为它训练了10个LOF作为子模型，可以观察一下它的速度差不多比LOF慢十倍。但值得注意的是Feature Bagging其实是一个模型框架，子模型可以用任何模型，比如HBOS等，因此效果可以提升。 HBOS和PCA的运算效率都很高，这个主要是因为模型比较简单。但也要值得注意PCA中需要逆矩阵，随着维度增加和实现不够高效可能很慢，所以和工具库的实现方法关系很大。同理，MCD和PCA虽然比较相似，但前者比后者慢很多。 LOF和KNN的表现相近，运算开销都相对较高，但在中小数据集上基本可以忽略这一点。 Isolation Forest的表现较差，但这个并不意外，因为我们测试的主要是中小数据集。随着数据量和维度的上升，Isolation Forest会比较有优势。 ABOD的运算效率很低，大部分情况下比KNN和LOF还慢，应该避免。 4. 总结首先我们要意识到这个实验的局限性： 选用的数据集普遍偏小（最大的不过50000条数据），因此不能完全反应出在极大数据量下的情况，比如Isolation Forest更适合的高维空间。我们也没有测试稀疏数据上的表现，这个有待验证。 为了实验的可重复性，固定了训练集和测试集（random_state=42）。但考虑到准确性，应该随机多次划分数据并求平均。 测试的模型有限，如果Pyod中引进了新的模型会更新这个结果。 算法的实现基础各不相同，比如有的底层是用C实现并经过优化，有的模型未经过优化，因此比较起来可能不大公平 所有模型均未调参，均是模型实现中的默认值。可能部分模型在经过精调以后有卓越的效果。 但即使这样，实验结果也带来很多异常检测模型选择时的思路： 不存在普遍意义上的最优模型，但有不少模型比较稳定，建议中小数据集上使用KNN或者MCD，中大型数据集上使用Isolation Forest。 模型效果和效率往往是对立的，比如PCA和MCD原理相似，但前者很快却效果一般，后者较慢但效果更好，因为后者比前者所考虑计算的更多，前者可以被认为是后者的简化版本。 因为一场检测往往是无监督的，稳定有时候比忽高忽低的性能更重要。所以即使HBOS运算速度很快，但因为效果时好时坏依然不是首选。 简单的模型不一定表现不好，比如KNN和MCD都属于非常简单的模型，但表现出众。 所以面对一个全新的异常检测问题，我个人认为可以遵循以下步骤分析： 我们队数据有多少了解，数据分布是什么样的，异常分布可能是什么样的。在了解这点后可以根据假设选择模型。 我们解决的问题是否有标签，如果有的话，我们应该优先使用监督学习来解决问题。标签信息非常宝贵，不要浪费。 如果可能的话，尝试多种不同的算法，尤其是我们对于数据的了解有限时。 可以根据数据的特点选择算法，比如中小数据集低纬度的情况下可以选择KNN，大数据集高维度时可以选择Isolation Forest。如果Isolation Forest的运算开销是个问题的话，也可以试试HBOS，在特征独立时可能有奇效。 无监督异常检测验证模型结果并不容易，可以采用半自动的方式：置信度高的自动放过，置信度低的人工审核。 意识到异常的趋势和特征往往处于变化过程中。比如明天的欺诈手段和今天的可能不同，因此需要不断的重新训练模型及调整策略。 不要完全依赖模型，尝试使用半自动化的策略：人工规则+检测模型。很多经验总结下来的人工规则很有用的，不要尝试一步到位的使用数据策略来代替现有规则。 5. 复现实验步骤（可选） 文中toy example的实验均直接取自开发者的GitHub（代码） 文中的对比实验也来自于开发者的GitHub（Jupyter Notebooks） 开发者提供了在线Jupyter Notebooks可以直接运行，无需安装，步骤： 点击Binder(beta) 等待Notebook初始化和载入 之后会默认进入根目录，有docs, examples, notebooks等文件夹，进入notebooks文件夹： 运行Compare All Models.ipynb 运行Benchmark.ipynb]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 作业调度]]></title>
    <url>%2Fspark%2Fspark-schedule%2F</url>
    <content type="text"><![CDATA[当谈及Spark中的调度时，往往容易使人迷惑。是指集群中多个spark运行程序的调度？还是指在一个spark程序内部不同任务的调度？于是，Spark调度可以分为两个层次：Spark应用间的调度和Spark应用内的调度。 Spark应用间的调度Spark提交作业到Yarn上，由Yarn来调度各作业间的关系。Yarn的调度策略可以为FAIR或者FIFO。在应用间的调度可以进一步分为两层，第一小层是Yarn的队列，第二小层是队列内的调度。Spark作业提交到不同的队列，通过设置不同的minishare、weight等，来实现不同作业调度的优先级，这一点Spark应用跟其他跑在Yarn上的应用没有区别，统一由Yarn公平调度。比较好的做法是每个用户单独一个队列，这种配置FAIR调度就是针对用户的了，可以防止恶意用户提交大量作业导致拖垮所有人的问题。这个配置在hadoop的yarn-site.xml里。1234&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;&lt;/property&gt; Spark应用内的调度在指定的Spark应用内即同一个SparkContext实例，多个线程可以并发地提交Spark作业（job）。作业是指由Spark action算子触发的一系列计算任务的集合。Spark调度器是完全线程安全的，并且能够支持Spark应用同时处理多个请求（比如：来自不同用户的查询）。这里简单列举一些调度管理模块涉及的相关概念： Task（任务）：单个分区数据集上的最小处理流程单元 TaskSet（任务集）：由一组关联的，但相互之间没有shuffle依赖关系的任务所组成的任务集。 Stage（调度阶段）：一个任务集对应的调度阶段。 Job（作业）：由一个RDD Action生成的一个或多个调度阶段所组成的一次计算作业。 Application（应用程序）：Spark应用程序，由一个或多个作业组成。 同一个SparkContext实例内部，可以配置该应用内的多个TaskSetManager间调度为FIFO还是FAIR。以Spark 的Thrift Server为例，考虑一个场景：用户a的作业很大，需要处理上T的数据，并且SQL也非常复杂，而用户b的作业很简单，只是select查看前面几条数据。由于用户a、b都在同一个SparkContext里，所以其调度完全由Spark决定；如果按FIFO的原则，可能用户b要等好一会才能从用户a的牙缝里扣出一点计算资源完成自己的作业，这样对用户b就不是那么友好了。比较好的做法是配置Spark应用内各个TaskSetManager的调度算法为FAIR，不需要等待用户a的资源，用户b的作业可以尽快得到执行。这里需要注意，FIFO并不是说用户b只能等待用户a所有task执行完毕才能执行，而只是执行的很迟，并且不可预料。从实测情况来看，配置为FIFO，用户b完成时间不一定，一般是4~6s左右；而配置为FAIR，用户b完成时间几乎是不变的，几百毫秒。应用内调度的配置项在{spark_base_dir}/conf/spark_default.conf: spark.scheduler.mode FAIR 应用内可以再分两层，第一小层是Pool（资源池）间的公平调度，第二小层是Pool内的。Pool内部调度默认是FIFO的，需要设置{spark_base_dir}/conf/fairscheduler.xml，针对具体的Pool设置调度规则。所以前面说TaskSetManager间调度不准确，应该是先Pool间再Pool内。下面配置default队列为FAIR调度1234567&lt;allocations&gt; &lt;pool name="default"&gt; &lt;schedulingMode&gt;FAIR&lt;/schedulingMode&gt; &lt;weight&gt;1&lt;/weight&gt; &lt;minShare&gt;3&lt;/minShare&gt; &lt;/pool&gt;&lt;/allocations&gt; 其中每个资源池都支持以下3个属性： schedulingMode：可以是FIFO或FAIR，控制资源池内部的作业是如何调度的。 weight：控制资源池相对其他资源池，可以分配到资源的比例。默认所有资源池的weight都是1。如果你将某个资源池的weight设为2，那么该资源池中的资源将是其他池子的2倍。如果将weight设得很高，如1000，可以实现资源池之间的调度优先级 – 也就是说，weight=1000的资源池总能立即启动其对应的作业。 minShare：除了整体weight之外，每个资源池还能指定一个最小资源分配值（CPU个数），管理员可能会需要这个设置。公平调度器总是会尝试优先满足所有活跃（active）资源池的最小资源分配值，然后再根据各个池子的weight来分配剩下的资源。因此，minShare属性能够确保每个资源池都能至少获得一定量的集群资源。minShare的默认值是0。 综上，如果你想要thriftserver达到SQL级别的公平调度，需要配置三个配置文件：yarn-site.xml、spark-defaults.conf、fairscheduler.xml。由于thriftserver的SQL没有按照不同用户区分多个Pool，所以其实还不算特别完整。 示例Spark可以并行运行多个计算。这可以通过在driver上启动多个线程并在每个线程中发出一组转换来轻松实现。然后，生成的任务将同时运行并共享应用程序的资源。这确保了资源永远不会保持空闲（例如，在等待特定转换的最后任务完成时）。默认情况下，任务以FIFO方式处理（在作业级别），但可以通过使用应用程序内调度程序来确保公平性（通过设置spark.scheduler.mode为FAIR）。然后，期望线程通过将spark.scheduler.pool本地属性（使用SparkContext.setLocalProperty）设置为适当的池名来设置其调度池。每个池分配的资源由配置spark.scheduler.allocation.file设置定义的XML文件确定（默认情况下，这是fairscheduler.xml在Spark的conf文件夹中）。12345678910111213141516171819import scala.concurrent.ExecutionContext.Implicits.globalimport scala.concurrent.duration._import scala.concurrent.&#123;Await, Future&#125;val spark = SparkSession .builder() .appName("Spatial data") .enableHiveSupport() .getOrCreate()val sc = spark.sparkContextsc.setLocalProperty("spark.scheduler.pool", "production")def input(i: Int) = sc.parallelize(1 to i * 100000)def serial = (1 to 10).map(i =&gt; input(i).reduce(_ + _)).sumdef parallel = (1 to 10).map(i =&gt; Future(input(i).reduce(_ + _))).map(Await.result(_, 10.minutes)).sum 多线程提交时，并未全部在指定的资源池中执行（原因不确定，需要进一步调研） The spark context is thread safe, so it’s possible to call it from many threads in parallel. (I am doing it in production) One thing to be aware of, is to limit the number of thread you have running, because: the executor memory is shared between all threads, and you might get OOM or constantly swap in and out memory from the cache the cpu is limited, so having more tasks than core won’t have any improvement Spark作业调度Job SchedulingTask调度算法，FIFO还是FAIROptimizing Spark jobs for maximum performance优化Spark作业以获得最佳性能Launching Apache Spark SQL jobs from multi-threaded driver]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ IDEA远程调试Spark应用程序]]></title>
    <url>%2FTools%2Fidea-spark%2F</url>
    <content type="text"><![CDATA[使用IDEA调试spark应用程序，是指使用spark算子编写的driver application。在开始之前，先介绍下如何使用idea远程debug普通的jar应用。远程debug spark原理是一样的。 远程debug普通的jar应用先假设远程debug的适用场景是：将应用程序打成jar包，让它运行在服务器上，然后在本地idea里以debug模式去运行这个jar包。希望达到的效果就像在idea里debug本地代码一样：可以断点，可以查看变量值等等。 我们将这个运行在服务器上的jar包称为被调试对象(debuggee)，本地IDEA称为调试者(debugger)。 远程调试有两种模式，或者说两种方式可选： attach模式：先运行debuggee，让其监听某个ip:port，然后等待debugger启动并连接这个端口，然后就可以在debugger上断点调试。 listen模式，让debugger监听某个ip:port，然后启动debuggee连接这个端口，接下来在debugger上断点调试。 attach模式在这种模式下，先运行debuggee，让其监听端口并等待debugger连接。在IDEA中的操作如下图： 复制-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005,这个是给debuggee的jvm参数。 其中将suspend=n，改成suspend=y，这样debuggee启动后会阻塞住直到debugger连接它 address=5005，表示debuggee监听这个端口，也可以指定成address=:的形式，这里ip是debuggee运行所在的机器的ip 上图中Host， Port应该和上面address中的ip，port一样，debugger会连接这个ip:port transport=dt_socket是debugger和debuggee之间传输协议 server=y, 在模式1下这样指定， 表示debuggee作为server等待debugger连接 在IDEA(debugger)里指定好这些之后，接下来就是先运行dubggee1java -cp ***.jar -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=10.9.233.114:5005 class 运行后出现下面信息：1Listening for transport dt_socket at address: 5005 表示debuggee等待连接。接下来的过程就是在idea里，设置断点，然后像本地debug一样了。 listen模式这种模式下debugger监听端口并等待debuggee的连接，所以需要先启动debugger。在IDEA里的操作如下： 该模式下先启动debugger，也就是启动idea调试，debugger会监听端口等待debuggee连接。按照上图复制给debuggee的jvm参数：1-agentlib:jdwp=transport=dt_socket,server=n,address=172.16.202.150:5005,suspend=y 这里去掉了onthrow=,onuncaught= 不知道是干什么的 address=ip:5005，该ip为debugger运行地址， debuggee连接该ip:port suspend=y, listen模式下可以去掉 server=n， 表示由debuggee发起连接到debugger。 此时debugger先运行并监听端口，接下来运行debuggee就可以了，如下：1java -cp ***.jar -agentlib:jdwp=transport=dt_socket,server=n,address=172.16.202.150:5005 class 调试SparkSpark按照角色可以分为Master、 Worker、Driver、Executor。其中Master, Worker只有在Standalone部署模式下才有，使用Yarn提交时只有Driver和Executor。使用Spark算子开发的应用提交执行后会都一个Driver和至少一个Executor，Driver充当job manager的角色，负责将RDD DAG划分为stage，创建task，调度task去executor执行等等。executor作为task executor执行算子。 调试Driver端相关代码Spark应用程序都有一个Driver，在—deploy-mode client模式在，Driver在启动程序的机器上运行。如果要调试driver端代码，需要在提交参数中设置driver的调试参数：1spark.driver.extraJavaOptions -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005 这个配置可以放在$SPARK_HOME/conf/spark-defaults.conf 里面；也可以在提交应用作业的时候设置1--driver-java-options &quot;-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005&quot; 这样就可以调试Driver相关代码了。 调试Executor端相关代码Driver只是一个job manager的角色，任务的执行（也就是那些spark 算子map, filter…的执行是在executor上执行的），即Spark应用程序的RDD内部计算逻辑都是在executor中完成的，所以如果需要在Executor启动的JVM加入相关的调试参数进行相关代码调试：1spark.executor.extraJavaOptions -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005 这个配置可以放在$SPARK_HOME/conf/spark-defaults.conf 里面；也可以在提交应用作业的时候设置1--conf &quot;spark.executor.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005&quot; executor对应的main方法所在类是org.apache.spark.executor.CoarseGrainedExecutorBackend。 提交细节由于一个job 可能有多个executor，而且在集群模式下会分布在不同的节点（服务器）上，不是很好调试。测试环境下应该可以设置为local模式，此时Driver也就是唯一可以启动的Executor。此时同时设置driver和executor调试参数，即可进行executor的调试了。1234567891011spark-submit --class com.zte.vmax.metadata.Coordinator \ --master local[*] \ --driver-java-options "-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005" \ --driver-memory 4G \ --executor-memory 10G \ --conf "spark.executor.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005" \ --files meta_task_2018011019_6725_lte_plan_capacity_addsite_turing36.json \ --jars lte_plan_capacity_addsite.jar \ /home/mr/testly/commonjar/metadata-coordinator_2.0.2-r4-SNAPSHOT.jar \ --config meta_task_2018011019_6725_lte_plan_capacity_addsite_turing36.json \ capacity.json 问题在调试 Executor 相关代码可能会遇到相关的问题。比如此时，需要将该处断点的属性设置为thread：右键该断点，在弹出窗口中选择Thread或者在debug栏中设置]]></content>
      <categories>
        <category>Tools</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【读书笔记之】朴素贝叶斯法]]></title>
    <url>%2Fmachinelearning%2Fnaive-bayes%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯（naive Bayes）法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入\输出的联合概率分布；然后基于此模型，对给定的输入$x$ ，利用贝叶斯定理求出后验概率最大的输出$y$。 朴素贝叶斯具有以下优点： 在测试数据集中预测很容易，也很快。在多类预测中也表现良好 当独立性假设成立时，Naive Bayes分类器较逻辑回归等其他模型表现更好，需要的训练数据更少 与数据变量相比，在特征为分类变量的情况下朴素贝叶斯表现良好。而对于数据变量，需要特征满足正态分布的假设（即钟型曲线，这是一个强假设） 同时朴素贝叶斯存在以下不足： 如果分类标量具有在训练数据集中未观察到的类别（在测试数据集中），则模型将制定0（零）概率并且将无法进行预测，这通常被称为“零概率”。为了解决这个问题， 我们可以使用平滑技术。其中，最简单的平滑技术之一称为拉普拉斯估计。 在另一方面朴素贝叶斯也被称为一个坏的估计，这样的概率输出形式predict_proba不应太认真对待。 朴素贝叶斯的另一个限制是特征变量独立性的假设。 在现实生活中，我们几乎不可能得到一组完全独立的预测变量。 在具体学习朴素贝叶斯法之前，我们先回顾下几个概念： 先验概率：指根据以往经验和分析得到的概率，即在未知条件下对事件发生可能性猜测的数学表示 后验概率：指事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小 算法思想朴素贝叶斯方法可以看成是以下流程的实现： 训练数据集——A——&gt;联合概率分布$p(x,y)$——B——&gt;预测类$y$A：特征条件独立B：最大化后验概率 求解方法极大似然估计贝叶斯估计]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【牛人博客之】精确率与召回率，RoC曲线与PR曲线]]></title>
    <url>%2Fmachinelearning%2Froc%2F</url>
    <content type="text"><![CDATA[在机器学习的算法评估中，尤其是分类算法评估中，我们经常听到精确率(precision)与召回率(recall)，RoC曲线与PR曲线这些概念，那这些概念到底有什么用处呢？ TP, FP, TN, FN刘建平 True Positives,TP：预测为正样本，实际也为正样本的特征数 False Positives,FP：预测为正样本，实际为负样本的特征数 True Negatives,TN：预测为负样本，实际也为负样本的特征数 False Negatives,FN：预测为负样本，实际为正样本的特征数 图如下所示，里面绿色的半圆就是TP(True Positives), 红色的半圆就是FP(False Positives), 左边的灰色长方形（不包括绿色半圆），就是FN（False Negatives）。右边的 浅灰色长方形（不包括红色半圆），就是TN(True Negatives)。这个绿色和红色组成的圆内代表我们分类得到模型结果认为是正值的样本。 精确率(precision),召回率(Recall)与特异性(specificity)精确率（Precision）的定义在上图可以看出，是绿色半圆除以红色绿色组成的圆。严格的数学定义如下： P = \frac{TP}{TP + FP }召回率(Recall)的定义也在图上能看出，是绿色半圆除以左边的长方形。严格的数学定义如下： R = \frac{TP}{TP + FN }特异性(specificity)的定义图上没有直接写明，这里给出，是右边长方形去掉红色半圆部分后除以右边的长方形。严格的数学定义如下： S = \frac{TN}{FP + TN }有时也用一个F1值来综合评估精确率和召回率，它是精确率和召回率的调和均值。当精确率和召回率都高时,F1值也会高。严格的数学定义如下： \frac{2}{F_1} = \frac{1}{P} + \frac{1}{R}有时候我们对精确率和召回率并不是一视同仁，比如有时候我们更加重视精确率。我们用一个参数$\beta$来度量两者之间的关系。如果$\beta&gt;1$, 召回率有更大影响，如果$\beta&lt;1$,精确率有更大影响。自然，当$\beta=1$的时候，精确率和召回率影响力相同，和$F1$形式一样。含有度量参数$\beta$的$F_1$我们记为$F{\beta}$, 严格的数学定义如下： F_\beta = \frac{(1+\beta^2)*P*R}{\beta^2*P + R}此外还有灵敏度(true positive rate ,TPR)，它是所有实际正例中，正确识别的正例比例，它和召回率的表达式没有区别。严格的数学定义如下： TPR = \frac{TP}{TP + FN }另一个是1-特异度(false positive rate, FPR)，它是实际负例中，错误得识别为正例的负例比例。严格的数学定义如下： FPR = \frac{FP}{FP + TN }我们熟悉了精确率， 召回率和特异性，以及TPR和FPR，后面的RoC曲线和PR曲线就好了解了。 RoC曲线和PR曲线有了上面精确率， 召回率和特异性的基础，理解RoC曲线和PR曲线就小菜一碟了。以TPR为y轴，以FPR为x轴，我们就直接得到了RoC曲线。从FPR和TPR的定义可以理解，TPR越高，FPR越小，我们的模型和算法就越高效。也就是画出来的RoC曲线越靠近左上越好。如下图左图所示。从几何的角度讲，RoC曲线下方的面积越大越大，则模型越优。所以有时候我们用RoC曲线下的面积，即AUC（Area Under Curve）值来作为算法和模型好坏的标准。 以精确率为y轴，以召回率为x轴，我们就得到了PR曲线。仍然从精确率和召回率的定义可以理解，精确率越高，召回率越高，我们的模型和算法就越高效。也就是画出来的PR曲线越靠近右上越好。如上图右图所示。使用RoC曲线和PR曲线，我们就能很方便的评估我们的模型的分类能力的优劣了。 刘建平. 精确率与召回率，RoC曲线与PR曲线 &#8617;]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【牛人博客之】交叉验证(Cross Validation)]]></title>
    <url>%2Fmachinelearning%2Fcross-validation%2F</url>
    <content type="text"><![CDATA[交叉验证是在机器学习建立模型和验证模型参数时常用的办法。交叉验证，顾名思义，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集合测试集，用训练集来训练模型，用测试集来评估模型预测的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的某样本在下次可能成为测试集中的样本，即所谓“交叉”。 那么什么时候才需要交叉验证呢？交叉验证用在数据不是很充足的时候。比如在我日常项目里面，对于普通适中问题，如果数据样本量小于一万条，我们就会采用交叉验证来训练优化选择模型。如果样本大于一万条的话，我们一般随机的把数据分成三份，一份为训练集（Training Set），一份为验证集（Validation Set），最后一份为测试集（Test Set）。用训练集来训练模型，用验证集来评估模型预测的好坏和选择模型及其对应的参数。把最终得到的模型再用于测试集，最终决定使用哪个模型以及对应参数。刘建平博客 回到交叉验证，根据切分的发放不同，交叉验证分为下面三种： 简单交叉验证：所谓的简单，是和其他交叉验证方法相对而言的。首先，我们随机的将样本数据分为两部分（比如：70%的训练集，30%的测试集），然后用训练集来训练模型，在测试集上验证模型及参数。接着，我们再把样本打乱，重新选择训练集和测试集，继续训练数据和检验模型。最后我们选择损失函数评估最优的模型和参数。 S折交叉验证（S-Folder Cross Validation）：和第一种方法不同，S折交叉验证会把样本数据随机的分成S份，每次随机的选择S-1份作为训练集，剩下的1份做测试集。当这一轮完成后，重新随机选择S-1份来训练数据。若干轮（小于S）之后，选择损失函数评估最优的模型和参数。 留一交叉验证（Leave-one-out Cross Validation）：它是第二种情况的特例，此时S等于样本数N，这样对于N个样本，每次选择N-1个样本来训练数据，留一个样本来验证模型预测的好坏。此方法主要用于样本量非常少的情况，比如对于普通适中问题，N小于50时，我一般采用留一交叉验证。 通过反复的交叉验证，用损失函数来度量得到的模型的好坏，最终我们可以得到一个较好的模型。那这三种情况，到底我们应该选择哪一种方法呢？一句话总结，如果我们只是对数据做一个初步的模型建立，不是要做深入分析的话，简单交叉验证就可以了。否则就用S折交叉验证。在样本量少的时候，使用S折交叉验证的特例留一交叉验证。 此外还有一种比较特殊的交叉验证方式，也是用于样本量少的时候。叫做自助法(bootstrapping)。比如我们有m个样本（m较小），每次在这m个样本中随机采集一个样本，放入训练集，采样完后把样本放回。这样重复采集m次，我们得到m个样本组成的训练集。当然，这m个样本中很有可能有重复的样本数据。同时，用没有被采样到的样本做测试集。这样接着进行交叉验证。由于我们的训练集有重复数据，这会改变数据的分布，因而训练结果会有估计偏差，因此，此种方法不是很常用，除非数据量真的很少，比如小于20个。 刘建平博客. 交叉验证(Cross Validation)原理小结 &#8617;]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【牛人博客之】最小二乘法]]></title>
    <url>%2Fmachinelearning%2Fleast-squares%2F</url>
    <content type="text"><![CDATA[最小二乘法是用来做函数拟合或者求函数极值的方法。在机器学习，尤其是回归模型中，经常可以看到最小二乘法的身影。 最小二乘法的原理与要解决的问题刘建平博客最小二乘法形式如下式： 目标函数 = \sum(观测值-理论值)^2观测值就是我们的样本，理论值就是我们的假设拟合函数（假设的模型输出）。目标函数也就是在机器学习中常说的损失函数，我们的目标是得到能够使目标函数最小化时候的拟合函数的模型。举一个最简单的线性回归的简单例子，比如我们有m个只有一个特征的样本： T=\left \{(x_1, y_1),(x_2, y_2),...,(x_m, y_m)\right \}样本采用下面的拟合函数： h_{\theta} (x)= \theta_0+ \theta_1x这样我们的样本有一个特征$x$，对应的拟合函数有两个参数$\theta_0$和$\theta_1$需要求出。我们的目标函数为： J(\theta_0, \theta_1)=\sum_{i=1}^{m} (y_i - h_{\theta}(x_i))^2 = \sum_{i=1}^{m}(y_i-\theta_0-\theta_1x_i)^2以上就是要解决的问题了，用最小二乘法做什么呢？使$J(\theta_0, \theta_1)$最小，求出使$J(\theta_0, \theta_1)$最小时的$\theta_0$和$\theta_1$，这样拟合函数就得出了。那么，最小二乘法怎么才能使$J(\theta_0, \theta_1)$最小呢？ $x_i$表示多个输入变量中的第$i$个，即有$n$维特征的第$i$个输入变量x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T$y_i$ 表示多个观察样本中的第$i$个 最小二乘法的解法代数解法为了使$J(\theta_0, \theta_1)$最小，方法就是对$\theta_0$和$\theta_1$分别来求偏导数，令偏导数为0，得到一个关于$\theta_0$和$\theta_1$的二元方程组。求解这个二元方程组，就可以得到$\theta_0$和$\theta_1$的值。下面我们具体看看过程。$J(\theta_0, \theta_1)$对$\theta_0$求导，得到如下方程： \frac{\partial J(\theta_0, \theta_1)}{\partial \theta_0}=-2 \sum_{i=1}^{m}(y_i - \theta_0 - \theta_1 x_i)令偏导数为0，得到如下方程： \sum_{i=1}^{m}(y_i - \theta_0 - \theta_1 x_i)=0 \tag{1.1}$J(\theta_0, \theta_1)$对$\theta_1$求导，得到如下方程： \frac{\partial J(\theta_0, \theta_1)}{\partial \theta_1}=-2 \sum_{i=1}^{m}(y_i - \theta_0 - \theta_1 x_i)x_i令该偏导数为0，则有： \sum_{i=1}^{m}(y_i - \theta_0 - \theta_1 x_i)x_i=0 \tag{1.2}公式(1.1)和(1.2)组成一个二元一次方程组，容易求出$\theta_1$的值： \theta_1=\frac{m \sum (x_iy_i) - \sum x_i \sum y_i}{m \sum x_i^2 - (\sum x_i)^2} \tag{1.3}将(1.3)代入(1.1)得到$\theta_0$的值： \theta_0=\frac{\sum x_i^2 \sum y_i - \sum x_i \sum x_iy_i}{m \sum x_i^2 - (\sum x_i)^2} \tag{1.4}这个方法很容易推广到多个样本特征的线性拟合。拟合函数表示为$h_\theta(x^{(1)},x^{(2)},…x^{(n)})=\theta_0 + \theta_1 x^{(1)}+…+\theta_n x^{(n)}$，其中$\theta_i(i=0, 1,2,…n)$为模型参数，$x^{(i)}(i=0, 1,2,…n)$为每个样本在第$i$个特征属性上的取值。这个表示可以简化，我们增加一个特征$x_0=1$，这样拟合函数表示为： h_\theta(x^{(1)},x^{(2)},...x^{(n)})=\sum_{i=0}^{n}\theta_i x_i损失函数表示为： J(\theta_0,\theta_1,...,\theta_n) = \sum_{j=1}^{m}(y_j - h_\theta(x_j^{(1)},x_j^{(2)},...x_j^{(n)}))^2 = \sum_{j=1}^{m} (y_j - \sum_{i=0}^{n} \theta_i x_j^i)^2利用损失函数分别对$\theta_i(i=0, 1,2,…n)$求导，并令导数为0可得： \sum_{j=0}^{m} (y_j - \sum_{i=0}^{n} \theta_ix_j^{(i)})x_j^{(i)} = 0这样我们得到一个$n+1$元一次方程组，这个方程组有$n+1$个方程，求解这个方程，就可以得到所有的$n+1$个未知的$\theta$。这个方法很容易推广到多个样本特征的非线性拟合。原理和上面的一样，都是用损失函数对各个参数求导取0，然后求解方程组得到参数值。 矩阵法解法矩阵法比袋鼠要简介，且矩阵运算可以取代循环，所以现在很多书和机器学习库都是用的矩阵法来做最小二乘法。这里用上面的多元线性回归例子来描述矩阵法解法。假设函数$h_\theta(x^{(1)},x^{(2)},…x^{(n)})=\theta_0 + \theta_1 x^{(1)}+…+\theta_n x^{(n)}$的矩阵表达方式为： h_\theta(X)= X\theta其中，假设函数$h_\theta(X)$为$m\times 1$的向量， $\theta$为$n \times 1$的向量，里面有$n$个代数法的模型参数。$X$为$m \times n$维的矩阵。$m$代表样本个数，$n$代表样本的特征数。损失函数定义为$J(\theta)=\frac{1}{2} (X \theta -Y)^T(X\theta-Y)$其中$Y$是样本的输出向量，维度为$m \times 1$，$\frac{1}{2}$在这里是为了求导后系数为1，方便计算。根据最小二乘法的原理，我们要对这个损失函数对$\theta$向量求导取0。结果如下式： \frac{\partial}{\partial \theta}J(\theta)=X^T(X\theta-Y)=0这里面用到了矩阵求导链式法则，和两个矩阵求导的公式： \frac{\partial}{\partial X} (XX^T)=2X \tag{2.1}\frac{\partial}{\partial \theta} (X\theta)=X^T \tag{2.2}对上述求导等式整理后可得： X^TX\theta = X^TY两边同时左乘$(X^TX)^{-1}$可得： \theta = (X^TX)^{-1}X^TY这样只要给了数据,我们就可以用$\theta = (X^TX)^{-1}X^TY$算出$\theta$ 最小二乘法的局限性和使用场景从上面可以看出，最小二乘法适用简洁高效，比梯度下降这样的迭代法似乎方便很多。但是这里我们就聊聊最小二乘法的局限性。 最小二乘法需要计算$X^TX$的逆矩阵，有可能它的逆矩阵不存在，这样就没有办法直接用最小二乘法了，此时梯度下降法仍然可以使用。当然，我们可以通过对样本数据进行整理，去掉冗余特征。让$X^TX$的行列式不为0，然后继续使用最小二乘法。 当样本特征$n$非常的大的时候，计算$X^TX$的逆矩阵是一个非常耗时的工作（$nxn$的矩阵求逆），甚至不可行。此时以梯度下降为代表的迭代法仍然可以使用。那这个$n$到底多大就不适合最小二乘法呢？如果你没有很多的分布式大数据计算资源，建议超过10000个特征就用迭代法吧。或者通过主成分分析降低特征的维度后再用最小二乘法。 如果拟合函数不是线性的，这时无法使用最小二乘法（为什么呢？），需要通过一些技巧转化为线性才能使用，此时梯度下降仍然可以用。 讲一些特殊情况。当样本量$m$很少，小于特征数$n$的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。当样本量$m$等于特征数$n$的时候，用方程组求解就可以了。当$m$大于$n$时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。 刘建平博客. 最小二乘法小结 &#8617;]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Code Segment]]></title>
    <url>%2Fspark%2FSpark-Code-Segment%2F</url>
    <content type="text"><![CDATA[RangePartitioner12345private sealed case class Item(value: Int)val rdd = sc.parallelize(1 to 4500).map(x =&gt; (Item(x), Item(x)))val partitioner = new RangePartitioner(1500, rdd)partitioner.getPartition(Item(100)) checkpoint spark 里面的 checkpoint 和 cache的区别There is a significant difference between cache and checkpoint. Cache materializes the RDD and keeps it in memory and/or disk. But the lineage of RDD (that is, seq of operations that generated the RDD) will be remembered, so that if there are node failures and parts of the cached RDDs are lost, they can be regenerated. However, checkpoint saves the RDD to an HDFS file and actually forgets the lineage completely. This is allows long lineages to be truncated and the data to be saved reliably in HDFS (which is naturally fault tolerant by replication). checkpoint 的正确使用姿势有一点要注意， 因为checkpoint是需要把 job 重新从头算一遍， 最好先cache一下， checkpoint就可以直接保存缓存中的 RDD 了， 就不需要重头计算一遍了， 对性能有极大的提升。使用很简单， 就是设置一下 checkpoint 目录，然后再rdd上调用 checkpoint 方法， action 的时候就对数据进行了 checkpoint1234val data = sc.textFile("/tmp/spark/1.data").cache() // 注意要cache sc.setCheckpointDir("/tmp/spark/checkpoint")data.checkpoint data.count checkpoint 将 RDD 持久化到 HDFS 或本地文件夹，如果不被手动 remove 掉，是一直存在的，也就是说可以被下一个 driver program 使用。 不同的 Locality Level PROCESS_LOCAL: 数据和 task 在同一个executor jvm 中，最好的就是这种 locality。 NODE_LOCAL: 数据在同一个节点上。比如数据在同一个节点的另一个 executor上；或在 HDFS 上，恰好有 block 在同一个节点上。速度比 PROCESS_LOCAL 稍慢，因为数据需要在不同进程之间传递或从文件中读取 NO_PREF: 数据从哪里访问都一样快，不需要位置优先 RACK_LOCAL: 数据在同一机架的不同节点上。需要通过网络传输数据及文件 IO，比 NODE_LOCAL 慢 ANY: 数据在非同一机架的网络上，速度最慢 举个例子， 假如 一个 task 要处理的数据，在上一个 stage 中缓存下来了， 这个 task 期望的 就是以 PROCESS_LOCAL 来运行， 这个时候缓存数据的executor 不巧正在执行 其他的task， 那么我就等一会， 等多长时间呢， spark.locality.wait.process这么长时间， 如果时间超了， executor 还是没有空闲下来， 那么我没有办法， 我就以NODE_LOCAL 来运行 task， 这个时候我想到 同一台机器上其他 executor 上跨jvm 去拉取数据， 如果同一台机器上有其他空闲的 executor 可以满足， 就这么干， 如果没有， 等待 spark.locality.wait.node 时间， 还没有就以更低的 Locality Level 去执行这个 task。 查看分区元素数工具1234567891011121314151617181920212223242526import org.apache.spark.SparkContextimport org.apache.spark.rdd.RDDimport scala.reflect.ClassTagobject RDDUtils extends Serializable &#123; def getPartitionCounts[T](sc : SparkContext, rdd : RDD[T]) : Array[Long] = &#123; sc.runJob(rdd, getIteratorSize _) &#125; def getIteratorSize[T](iterator: Iterator[T]): Long = &#123; var count = 0L while (iterator.hasNext) &#123; count += 1L iterator.next() &#125; count &#125; def collectPartitions[T: ClassTag](sc: SparkContext, rdd: RDD[T]): Array[Array[T]] = &#123; sc.runJob(rdd, (iter: Iterator[T]) =&gt; iter.toArray) &#125;&#125;val rdd = sc.parallelize(Array(("A",1),("A",1),("A",1),("b",1),("b",1)), 5)RDDUtils.getPartitionCounts(sc, rdd).foreach(println)RDDUtils.collectPartitions(sc, rdd) glom函数该函数是将RDD中每一个分区中类型为T的元素转换成Array[T]，这样每一个分区就只有一个数组元素。123456789scala&gt; var rdd = sc.makeRDD(1 to 10,3)rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[38] at makeRDD at :21scala&gt; rdd.partitions.sizeres33: Int = 3 //该RDD有3个分区scala&gt; rdd.glom().collectres35: Array[Array[Int]] = Array(Array(1, 2, 3), Array(4, 5, 6), Array(7, 8, 9, 10))//glom将每个分区中的元素放到一个数组中，这样，结果就变成了3个数组 map VS mapPartitionsmapPartitions函数和map函数类似，只不过映射函数的参数由RDD中的每一个元素变成了RDD中每一个分区的迭代器。假设一个rdd有10个元素，分成3个分区。如果使用map方法，map中的输入函数会被调用10次；而使用mapPartitions方法的话，其输入函数会只会被调用3次，每个分区调用1次。123456789101112131415161718//生成10个元素3个分区的rdd a，元素值为1~10的整数（1 2 3 4 5 6 7 8 9 10），sc为SparkContext对象val a = sc.parallelize(1 to 10, 3)//定义两个输入变换函数，它们的作用均是将rdd a中的元素值翻倍//map的输入函数，其参数e为rdd元素值 def myfuncPerElement(e:Int):Int = &#123; println("e="+e) e*2 &#125; //mapPartitions的输入函数。iter是分区中元素的迭代子，返回类型也要是迭代子def myfuncPerPartition ( iter : Iterator [Int] ) : Iterator [Int] = &#123; println("run in partition") var res = for (e &lt;- iter ) yield e*2 res&#125;val b = a.map(myfuncPerElement).collectval c = a.mapPartitions(myfuncPerPartition).collect// 可看到打印了3次run in partition，打印了10次e= 从输入函数（myfuncPerElement、myfuncPerPartition）层面来看，map是推模式，数据被推到myfuncPerElement中；mapPartitons是拉模式，myfuncPerPartition通过迭代子从分区中拉数据。这两个方法的另一个区别是在大数据集情况下的资源初始化开销和批处理处理，如果在myfuncPerPartition和myfuncPerElement中都要初始化一个耗时的资源，然后使用，比如数据库连接。在上面的例子中，myfuncPerPartition只需初始化3个资源（3个分区每个1次），而myfuncPerElement要初始化10次（10个元素每个1次），显然在大数据集情况下（数据集中元素个数远大于分区数），mapPartitons的开销要小很多，也便于进行批处理操作。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpatialSpark源码阅读]]></title>
    <url>%2Fspark%2Fspatial-spark%2F</url>
    <content type="text"><![CDATA[构建spatialRDD基于key-value形式的RDD生成SpatialRDD类SpatialRDD继承自spark-core的RDD伴生对象SpatialRDD调用updatable函数对k-v RDD进行构造。其中对重复key进行了任意合并，具体怎么实现呢？？？定义了函数z和f，而函数z和f在elems上的应用则是在再次调用的updatable中定义，该现象为scala高阶函数2中的一个函数作为另一个函数的参数1234567891011121314151617/** * Constructs an updatable IndexedRDD from an RDD of pairs, merging duplicate keys arbitrarily. 任意合并重复key怎么体现？？？ */def updatable[K: ClassTag, V: ClassTag](elems: RDD[(K, V)]): SpatialRDD[K, V] = updatable[K, V, V](elems, z = (id, a) =&gt; a, f = (id, a, b) =&gt; b) //定义了函数z和f，而函数z和f在elems上的应用则是在updatable中定义，该现象为高阶函数中的一个函数作为另一个函数的参数 /** * Constructs an SpatialRDD from an RDD of pairs. * the default partitioner is the quadtree based partioner */def updatable[K: ClassTag, U: ClassTag, V: ClassTag](elems: RDD[(K, V)], z: (K, U) =&gt; V, f: (K, V, U) =&gt; V): SpatialRDD[K, V] = &#123; val elemsPartitioned = elems.partitionBy(new QtreePartitioner(Util.numPartition, Util.sampleRatio, elems)) val partitions = elemsPartitioned.mapPartitions[SpatialRDDPartition[K, V]]( iter =&gt; Iterator(RtreePartition(iter, z, f)), preservesPartitioning = true) new SpatialRDD(partitions) &#125; 在对key-valueRDD进行数据切割的时候，重新定义了一个分区函数QtreePartitioner，类似于spark原生的HashPatitionner（哈希分区）和RangePatitioner（区域分区），既决定了RDD本身的分区数量，也可以作为其父RDD Shuffle输出（MapOutput）中每个分区进行数据切割的依据。 新的分区函数QtreePartitionerSpark内部提供了HashPartitioner和RangePartitioner两种分区策略，这两种分区策略在很多情况下都适合我们的场景。但是有些情况下，Spark内部不能符合需求，这时候就可以自定义分区策略。为此，Spark提供了相应的接口，我们只需要扩展Partitioner抽象类，然后实现里面的三个方法：12345678910package org.apache.spark/** * An object that defines how the elements in a key-value pair RDD are partitioned by key. * Maps each key to a partition ID, from 0 to `numPartitions - 1`. */abstract class Partitioner extends Serializable &#123; def numPartitions: Int def getPartition(key: Any): Int&#125; def numPartitions: Int：这个方法需要返回你想要创建分区的个数；def getPartition(key: Any): Int：这个函数需要对输入的key做计算，然后返回该key的分区ID，范围一定是0到numPartitions-1，分区ID相同的数据元素将被分配到同一个数据分片中；equals()：这个是Java标准的判断相等的函数，之所以要求用户实现这个函数是因为Spark内部会比较两个RDD的分区是否一样。Spark自定义分区(Partitioner) 1234567891011121314151617181920212223242526272829303132333435363738val quadtree: QtreeForPartion = &#123; val total = rdd.count() // 不同数据量的采样比例不同 val fraction2 = if (total * fraction &gt; 5e5) (5e5 / total).toFloat else fraction //对key值进行采样：true 泊松采样，有放回抽样； false 伯努利采样，无放回抽样；fraction 抽样比例 var sampleData = rdd.map(_._1).sample(false, fraction2).collect() //in case the sample data size is too small,expand the sample ratio 50 times. if (sampleData.length &lt; 10000) &#123; sampleData = rdd.map(_._1).sample(false, 0.2).collect() &#125; //整数除法：每个数据分片分配的抽样数据个数，即叶子节点的大小，是否每个数据分片都能够有数据 var leafBound = sampleData.length / partitions if (leafBound == 0) &#123; leafBound = qtreeUtil.leafbound &#125; val qtree = new QtreeForPartion(leafBound) // 为什么只有采样数据？？？ sampleData.foreach &#123; case p: Point =&gt; qtree.insertPoint(p) case _ =&gt; println("do not support this data type") &#125; realnumPartitions = qtree.computePIDofLeaf(sampleData.length, partitions) //println("bound "+leafbound) //qtree.printTreeStructure() qtree&#125; Spark自定义分区(Partitioner). https://www.iteblog.com/archives/1368.html &#8617;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>QuadTree</tag>
        <tag>R-Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[空间的索引结构]]></title>
    <url>%2Falgorithm%2Fspatial-index%2F</url>
    <content type="text"><![CDATA[空间查询 最邻近查询：给定一个点或者对象，查询满足条件的最近对象 区域查询：查询全部或者部分落在限定区域内的对象 空间索引结构基于空间分割的索引结构 Grid-based Quad-tree四叉树索引的基本思想是将地理空间递归划分为不同层次的树结构。它将已知范围的空间等分成四个相等的子空间，如此递归下去，直至树的层次达到一定深度或者满足某种要求后停止分割。四叉树的结构比较简单，并且当空间数据对象分布比较均匀时，具有比较高的空间数据插入和查询效率，因此四叉树是GIS中常用的空间索引之一。常规四叉树的结构如图所示，地理空间对象都存储在叶子节点上，中间节点以及根节点不存储地理空间对象。四叉树对于区域查询，效率比较高。但如果空间对象分布不均匀，随着地理空间对象的不断插入，四叉树的层次会不断地加深，将形成一棵严重不平衡的四叉树，那么每次查询的深度将大大的增多，从而导致查询效率的急剧下降。 四叉树结构是自顶向下逐步划分的一种树状的层次结构。传统的四叉树索引存在着以下几个缺点： 空间实体只能存储在叶子节点中，中间节点以及根节点不能存储空间实体信息，随着空间对象的不断插入，最终会导致四叉树树的层次比较深，在进行空间数据窗口查询的时候效率会比较低下。 同一个地理实体在四叉树的分裂过程中极有可能存储在多个节点中，这样就导致了索引存储空间的浪费。 由于地理空间对象可能分布不均衡，这样会导致常规四叉树生成一棵极为不平衡的树，这样也会造成树结构的不平衡以及存储空间的浪费。 相应的改进方法，将地理实体信息存储在完全包含它的最小矩形节点中，不存储在它的父节点中，每个地理实体只在树中存储一次，避免存储空间的浪费。首先生成满四叉树，避免在地理实体插入时需要重新分配内存，加快插入的速度，最后将空的节点所占内存空间释放掉。改进后的四叉树结构如下图所示。四叉树的深度一般取经验值4-7之间为最佳。 k-D tree 数据驱动的索引结构R-TreeR树很好的解决了高维空间搜索问题。它把B树的思想扩展到多维空间，采用了B树分割空间的思想，并在添加、删除操作时采用合并、分解结点的方法，保证树的平衡性。因此，R树就是一棵用来存储高维数据的平衡树。 R树的数据结构R树是B树在高维空间的扩展，是一棵平衡树。每个R树的叶子结点包含了多个指向不同数据的指针，这些数据可以是存放在硬盘中的，也可以是存在内存中。根据R树的这种数据结构，当我们需要进行一个高维空间查询时，我们只需要遍历少数几个叶子结点所包含的指针，查看这些指针指向的数据是否满足要求即可。这种方式使我们不必遍历所有数据即可获得答案，效率显著提高。下图1是R树的一个简单实例：R树采用了一种称为MBR(最小边界矩形，Minimal Bounding Rectangle)的方法进行空间分割。从叶子结点开始用矩形（rectangle）将空间框起来，结点越往上，框住的空间就越大，以此对空间进行分割。我们就拿二维空间来举例。下图是Guttman论文中的一幅图： 首先我们假设所有数据都是二维空间下的点，图中仅仅标志了R8区域中的数据，也就是那个shape of data object。别把那一块不规则图形看成一个数据，我们把它看作是多个数据围成的一个区域。为了实现R树结构，我们用一个最小边界矩形恰好框住这个不规则区域，这样，我们就构造出了一个区域：R8。R8的特点很明显，就是正正好好框住所有在此区域中的数据。其他实线包围住的区域，如R9，R10，R12等都是同样的道理。这样一来，我们一共得到了12个最最基本的最小矩形。这些矩形都将被存储在子结点中。 下一步操作就是进行高一层次的处理。我们发现R8，R9，R10三个矩形距离最为靠近，因此就可以用一个更大的矩形R3恰好框住这3个矩形。 同样道理，R15，R16被R6恰好框住，R11，R12被R4恰好框住，等等。所有最基本的最小边界矩形被框入更大的矩形中之后，再次迭代，用更大的框去框住这些矩形。 我想大家都应该理解这个数据结构的特征了。用地图的例子来解释，就是所有的数据都是餐厅所对应的地点，先把相邻的餐厅划分到同一块区域，划分好所有餐厅之后，再把邻近的区域划分到更大的区域，划分完毕后再次进行更高层次的划分，直到划分到只剩下两个最大的区域为止。要查找的时候就方便了。下面就可以把这些大大小小的矩形存入我们的R树中去了。根结点存放的是两个最大的矩形，这两个最大的矩形框住了所有的剩余的矩形，当然也就框住了所有的数据。下一层的结点存放了次大的矩形，这些矩形缩小了范围。每个叶子结点都是存放的最小的矩形，这些矩形中可能包含有n个数据。 GeoSparkSpatialSparkSpatialSpark和SparkDistributedMatrix调研小结Spatial Database城市计算轨迹数据挖掘]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git]]></title>
    <url>%2Ftool%2Fgit%2F</url>
    <content type="text"><![CDATA[更改远程origingit remote －v 查看远程origingit remote rm origin 删除远程origingit remote add origin https://github.com/***/***.gitgit push －u origin master //第一次push的时候要- ugit remote add originTwo https://github.com/***/***.git //更改远程名称为originTwo]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[空间数据库]]></title>
    <url>%2Funcategorized%2Fspatial-database%2F</url>
    <content type="text"><![CDATA[空间数据库是一个被优化的数据库，用于存储和查询代表几何空间对象的数据。多数空间数据库能够支持点、线及多边形这些简单的几何对象。有些空间数据库则可以处理3D、拓扑覆盖、线性网络以及TIN等更复杂的几何结构1。数据库系统使用索引来实现快速查询。普通数据库的索引数据方式在进行空间查询时不够优化，因此空间数据库使用空间索引来加速对数据库的操作。 除了典型的SQL查询，比如SELECT查询之外，空间数据库可以支持更多的空间操作。以下几种是由Open Geospatial Consortium标准定义的空间数据库操作： 空间测量：计算线的长度，多边形的面积，两个几何对象间的距离等等 空间函数：通过修改已有特征来构造新的几何对象 空间预测：支持几何体之间的空间关系的真或者假查询，比如，两个多边形有重叠或者在打算建填埋场的这个区域的一公里内是否有人居住？ 几何构造：在可以定义形状的边或者定点已知的情况下，构造新的几何结构。 查询函数：该查询返回特定信息，比如一个圆中心点的位置 空间索引空间索引用于空间数据库的优化查询。 普通的索引类型无法满足空间查询的效率，比如两点的差异有多大，或者某些点是否在这个空间区域范围内等。一般的空间索引包括： Geohash HHCode Grid (spatial index) Z-order (curve) Quadtree Octree UB-tree R-tree: 通常是索引空间数据的首选方法。 使用最小边界矩形（MBR）对对象（形状，线和点）进行分组。 对象被添加到索引中的MBR中，这将导致其大小的最小增加。 R+ tree R* tree Hilbert R-tree X-tree kd-tree m-tree: 当使用任意度量进行比较时，m树索引可用于有效地解决复杂对象上的相似性查询 Point access method Binary space partitioning (BSP-Tree): Subdividing space by hyperplanes. 1.https://en.wikipedia.org/wiki/Spatial_database ↩]]></content>
      <tags>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark数据持久化与释放]]></title>
    <url>%2Fspark%2Fspark_cache%2F</url>
    <content type="text"><![CDATA[在日常的Spark应用开发过程中，对多次使用到的数据往往会进行持久化，即将数据从HDFS中加载到内存中，这样在后续应用中不用反复从HDFS中读取数据，可以提升数据加载速度。 持久化cache和persist无效的cache/persistcache/persist后的rdd，没有使用就unpersist，等于白干! 12345678910val rdd1 = ... // 读取hdfs数据，加载成RDDrdd1.cacheval rdd2 = rdd1.map(...)val rdd3 = rdd1.filter(...)rdd1.unpersistrdd2.take(10).foreach(println)rdd3.take(10).foreach(println) 上面代码的意图是：既然rdd1会被利用两次，那么就缓存起来，用完后释放内存。问题是，rdd1还没有被复用，就被“释放”了，导致rdd2,rdd3在执行take时，仍然需要从hdfs中加载rdd1,没有到达cache效果。 原理这里要从RDD的操作谈起，RDD的操作分为两类：action和tranformation。区别是tranformation输入RDD，输出RDD，而action输入RDD，输出非RDD。transformation是缓释执行的，action是即刻执行的。上面的代码中，hdfs加载数据，map，filter都是transformation，take是action。所以当rdd1加载时，并没有被调用，直到take调用时，rdd1才会被真正的加载到内存。 cache和unpersisit两个操作比较特殊，他们既不是action也不是transformation。cache会将标记需要缓存的rdd，真正缓存是在第一次被相关action调用后才缓存；unpersisit是抹掉该标记，并且立刻释放内存。 所以，综合上面两点，可以发现，在rdd2的take执行之前，rdd1，rdd2均不在内存，但是rdd1被标记和剔除标记，等于没有标记。所以当rdd2执行take时，虽然加载了rdd1，但是并不会缓存。然后，当rdd3执行take时，需要重新加载rdd1，导致rdd1.cache并没有达到应该有的作用，所以，正确的做法是将take提前到unpersist之前，如下：12345678910val rdd1 = ... // 读取hdfs数据，加载成RDDrdd1.cacheval rdd2 = rdd1.map(...)val rdd3 = rdd1.filter(...)rdd2.take(10).foreach(println)rdd3.take(10).foreach(println)rdd1.unpersist 这样，rdd2执行take时，会先缓存rdd1，接下来直接rdd3执行take时，直接利用缓存的rdd1，最后，释放掉rdd1。 释放被持久化的RDD123456789101112131415161718192021val spark = SparkSession .builder .appName("Global Unpersist cached RDD") .getOrCreate()val sc = spark.sparkContextval rdd1 = sc.makeRDD(1 to 100)val rdd2 = sc.makeRDD(10 to 1000)rdd1.cache.setName("foo")rdd2.cache.setName("rdd_2")rdd1.count()rdd2.count()//单独释放一个RDDrdd1.unpersist()// 获得所有持久化的RDD，并进行指定释放val rdds = sc.getPersistentRDDsrdds.filter(x =&gt; x._2.name.contains("rdd")).foreach(x =&gt; x._2.unpersist()) Spark: list all cached RDD namesSpark: unpersist RDDs for which I have lost the reference]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每日一问]]></title>
    <url>%2Fdatascience%2Fdaily_question%2F</url>
    <content type="text"><![CDATA[2018-102018-10-18Q: How can you check if a data set or time series is Random? A: To check whether a data set is random or not, use the lag plot. If the lag plot for the given data set does not show any structure then it is random.A lag plot checks whether a data set or time series is random or not. Random data should not exhibit any identifiable structure in the lag plot. Non-random structure in the lag plot indicates that the underlying data are not random. Several common patterns for lag plots are shown in the examplesbelow. 2018-10-19Question: You are working on a time series data set. Your manager has asked you to build a high accuracy model. You start with the decision tree algorithm, since you know it works fairly well on all kinds of data. Later, you tried a time series regression model and got higher accuracy than decision tree model. Can this heppen? Why? Answer: Time series data is known to posses linearity. On the other hand, a decision tree algorithm is known to work best to detect non-linear interactions. The reason why decision tree failed to provide robust predictions because it couldn’t map the linear relationship as good as a regression model did. Therefore, we learned that, a linear regression model can provide robust prediction given the data set satisfies its linearity assumptions. 2018-10-20Question: What is seasonality in time series modelling? Answer:Seasonality in time series occurs when time series shows a repeated pattern over time. E.g., stationary sales decreases during holiday season, air conditioner sales increases during the summers etc. are few examples of seasonality in a time series.Seasonality makes your time series non-stationary because average value of the variables at different time periods. Differentiating a time series is generally known as the best method of removing seasonality from a time series. Seasonal differencing can be defined as a numerical difference between a particular value and a value with a periodic lag. 2018-10-21Question: Give some classification situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa.Answer: When the data is outlier free and clean then go for SVM. If your data might contain outliers then Random forest would be the best choice Generally, SVM consumes more computational power than Random Forest, so if you are constrained with memory go for Random Forest machine learning algorithm. Random Forest gives you a very good idea of variable importance in your data, so if you want to have variable importance then choose Random Forest machine learning algorithm. Random Forest machine learning algorithms are preferred for multiclass problems. SVM is preferred in multi-dimensional problem set - like text classification 2018-10-22Question: How to define the number of clusters?Answer: The elbow methodThis method looks at the percentage of variance explained as a function of the number of clusters: choose a number of clusters so that adding another wouldn’t add significant information to modeling.X-means clusteringA variation of k-means clustering that refines cluster assignments by repeatedly attempting optimal subdivision, until a criteria such as AIC or BIC is reached.Cross ValidationPartition the data into k folds, and each of the folds is set aside at turn as a test set. A clustering model is then computed on the other k − 1 training sets, and the value of the objective function (for example, the sum of the squared distances to the centroids for k-means) calculated for the test set. Compare the averages of these k values for each alternative number of clusters, and select the number of cluster such that a further increase leads to only a small reduction in the objective function. Reference: https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set 2018-10-23Question: A/B测试有什么作用?Answer: 它是对具有两个变量A和B的随机实验的统计假设检验.A / B测试的目标是识别网页的任何变化以最大化或增加收益的结果。 一个例子可以是识别横幅广告的点击率。 2018-10-24Question: 什么是Boosting和Stacking，两者有什么不同?Answer: Boosting提供预测变量的顺序学习。 第一个预测器是在整个数据集上学习的，而后续的预测器则是基于前一个预测器的结果在训练集上学习的。 它首先对原始数据集进行分类，并为每个观察值赋予相同的权重。 如果使用第一个学习器预测分类错误，那么给予分错类的观察样例更高的权重。 作为一个迭代过程，它继续添加分类器学习器，直到达到模型数量或精度的限制。 Boosting显示出比Bagging更好的预测准确性，但它也倾向于过度拟合训练数据。 Stacking 分为两个阶段。 首先，我们使用多个基本分类器来预测类。 其次，将基分类器的预测组合起来作为一个新学习器，以减少泛化错误。 2018-10-25Question:朴素贝叶斯的优点和缺点是什么？Answer: 优点: 预测测试数据集类很容易，也很快。 它在多类预测中也表现良好。 当独立性假设成立时，Naive Bayes分类器与逻辑回归等其他模型相比表现更好，需要的训练数据更少。 与数值变量相比，它在分类输入变量的情况下表现良好。 对于数值变量，假设正态分布（钟形曲线，这是一个强有力的假设） 缺点: 如果分类变量具有在训练数据集中未观察到的类别（在测试数据集中），则模型将指定0（零）概率并且将无法进行预测。 这通常被称为“零频率”。 为了解决这个问题，我们可以使用平滑技术。 最简单的平滑技术之一称为拉普拉斯估计。 在另一方面朴素贝叶斯也被称为一个坏的估计，这样的概率输出形式predict_proba不应太认真对待。 朴素贝叶斯的另一个限制是预测变量独立性的假设。 在现实生活中，我们几乎不可能得到一组完全独立的预测变量。 2018-10-26Question: 给我一些关于Naive Bayes算法应用的例子。Answer: 实时预测：朴素贝叶斯是一个热切的学习分类器，它确实很快。 因此，它可以用于实时预测。 多类预测：该算法对于多类预测特征也是众所周知的。 在这里，我们可以预测多类目标变量的概率。 文本分类/垃圾邮件过滤/情感分析：与其他算法相比，主要用于文本分类的朴素贝叶斯分类器（由于更好地导致多类问题和独立性规则）具有更高的成功率。 因此，它被广泛用于垃圾邮件过滤（识别垃圾邮件）和情感分析（在社交媒体分析中，以识别积极和消极的客户情绪） 推荐系统：朴素贝叶斯分类器和协同过滤一起构建一个推荐系统，该系统使用机器学习和数据挖掘技术来过滤看不见的信息并预测用户是否想要一个给定的资源 2018-10-27 Question: 描述什么是决策树算法 Answer: 决策树是一种监督学习算法。 它适用于分类和连续输入和输出变量。 在该技术中，我们基于输入变量中最重要的分裂器/微分器将群体或样本分成两个或更多个同构集（或子群） 2018-10-30 Question: Whats’s the advantages and disadvantages of decision tree? Answer: Advantage: 易于理解：决策树输出非常容易理解。 它的图形表示非常直观，用户可以轻松地将他们的假设联系起来。 在数据探索中很有用：决策树是识别最重要变量和两个或多个变量之间关系的最快方法之一。 在决策树的帮助下，我们可以创建具有更好预测目标变量能力的新变量/特征。 它也可以用于数据探索阶段。 例如，我们正在研究一个问题，即我们有数百个变量可用的信息，决策树将有助于识别最重要的变量。 需要更少的数据清理：与其他一些建模技术相比，它需要更少的数据清理。 它不受异常值和缺失值的影响。 数据类型不是约束：它可以处理数字和分类变量。 非参数方法：决策树被认为是非参数方法。 这意味着决策树没有关于空间分布和分类器结构的假设。 Disadvantages: 过度拟合：过度拟合是决策树模型最实际的难点之一。 通过设置模型参数和修剪的约束来解决此问题。 不适合连续变量：在处理连续数值变量时，决策树在对不同类别的变量进行分类时会丢失信息。 2018-10-31Question: What are the primary differences &amp; similarity between classification and regression tree?Answer: 当因变量是连续的时，使用回归树。 因变量是分类时使用分类树。 在回归树的情况下，训练数据中终端节点获得的值是落在该区域中的观察的平均响应。 因此，如果看不见的数据观察属于该区域，我们将使用平均值进行预测。 在分类树的情况下，终端节点在训练数据中获得的值（类）是落在该区域中的观察模式。 因此，如果看不见的数据观察属于该区域，我们将使用模式值进行预测。 两棵树都将预测空间（自变量）划分为不同的和不重叠的区域。 为简单起见，您可以将这些区域视为高维盒子或盒子。 树都遵循自上而下的贪婪方法，称为递归二进制分裂。 我们将其称为“自上而下”，因为当所有观察在单个区域中可用时，它从树的顶部开始，并且连续地将预测器空间分成树下的两个新分支。 它被称为’贪婪’，因为该算法仅关注当前的分裂，而不关心将来会导致更好的树的分裂。 继续该拆分过程，直到达到用户定义的停止标准。 在这两种情况下，分裂过程都会导致树木完全生长，直到达到停止标准。 但是，完全成长的树可能会过度填充数据，导致看不见的数据的准确性很差。 2018-112018-11-01Question: How does a tree decide where to split?Answer: The decision criteria is different for classification and regression trees.分类和回归树的决策标准不同。Decision trees use multiple algorithms to decide to split a node in two or more sub-nodes. The creation of sub-nodes increases the homogeneity of resultant sub-nodes.决策树使用多种算法来决定将节点拆分为两个或更多个子节点。 子节点的创建增加了所得子节点的同质性。Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes. Some most commonly used algorithms to split the node are: Gini Index, Chi-Square, Information Gain, Reduction in Variance.决策树在所有可用变量上拆分节点，然后选择导致最同质子节点的拆分。 一些最常用的分割节点的算法是：基尼指数，卡方，信息增益，方差减少。 2018-11-02Question: Tell me some majors issues needed to be considered in supervised machine learning?告诉我一些在监督机器学习中需要考虑的专业问题？Answer: Bias-variance tradeoff偏差 - 方差权衡 Function complexity and amount of training data功能复杂性和训练数据量 Dimensionality of the input space输入空间的维度 Noise in the out put values输出值中的噪声 Input data problems such as Heterogeneity of the data. Redundancy in the data and Presence of interactions and non-linearities.输入数据问题，例如数据的异构性。 数据中的冗余以及交互和非线性的存在 Reference: https://en.wikipedia.org/wiki/Supervised_learning 2018-11-03Question: What are methods to make a predictive model robust to outliers?有哪些方法可以使预测模型对异常值具有鲁棒性Answer: Use a model that is resistant to outliers. Tree-based models are not as affected by outliers as regression models. For statistical tests, choose non -parametric test instead of parametric test使用对异常值有抵抗力的模型。 基于树的模型不像回归模型那样受到异常值的影响。 对于统计测试，请选择非参数测试而不是参数测试 Use a more robust error metric. For instance, use absolute mean difference instead of mean squared error to reduce the effect of outliers使用更强大的错误指标。 例如，使用绝对平均差而不是均方误差来减少异常值的影响 Winsorize the data. Cap the data at a certain threshold对数据进行Winsorize。 将数据限制在特定阈值 Transform the data. If the data has a pronounced right tail, use log transform转换数据。 如果数据具有明显的右尾，请使用log 转换 Remove the outliers. If there are very few of outliers and you are certain that they are anomalies not worth predicting删除异常值。 如果极少数异常值并且您确定它们是不值得预测的异常 Reference: https://www.quora.com/What-are-methods-to-make-a-predictive-model-more-robust-to-outliers 2018-11-06Question: How does the KNN algorithm work?Answer: KNN是一种有监督的机器学习算法，基本上它基于从查询实例到训练样本的最小距离来确定K-最近邻居是查询实例的预测。 它可以用于分类和回归问题。 2018-11-07Question: How do we choose the factor K?Answer: 较小的K将导致结果有很大的差异，较大的K将使每个点分类为最可能的类。 因此，我们需要做的是将一部分训练数据留作验证集，然后更改不同数量的K以查看哪一个能够获得最佳性能。 2018-11-08Question: Talk about how K-D Tree improve the KNNAnswer: k-d树是二叉树，其中每个节点是k维点。 每个非叶节点都可以被认为是隐式生成一个分裂超平面，它将空间划分为两个部分，称为半空间。 该超平面左侧的点由该节点的左子树表示，超平面右侧的点由右子树表示。 超平面方向按以下方式选择：树中的每个节点与k维度中的一个相关联，超平面垂直于该维度的轴。 基本上，KD Tree将数据集划分为几个区域，这次当我们计算查询实例的距离时，我们计算它到所有其他数据点的距离，我们只计算在同一区域内的数据的距离 使用查询实例，从而减少计算时间的复杂性。 2018-11-09Question: Describe the basic steps to do the PCA(Principal Components Analysis)Answer: 标准化数据 从协方差矩阵获得特征向量和特征值用于相关矩阵，或执行 奇异向量分解。 按降序对特征值进行排序，并选择与k个最大特征值对应的k个特征向量，其中k是新特征子空间的维数 从所选择的k个本征向量构造投影矩阵W. 通过W变换原始数据集X以获得k-维度特征子空间Y. 2018-11-10Question: What is Random Projection?什么是随机投影Answer: It is a unsupervised machine learning method to do the dimension reduction.这是一种无监督的机器学习方法来进行降维。It creates a minimum reduced dimension k which can make the new pairwise data distance preserved within an accepted error comparing to the original pairwise data distance它创建了最小缩减尺寸k，这可以使新的成对数据距离与原始成对数据距离相比保持在可接受的误差内]]></content>
      <categories>
        <category>Data Science</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[集成学习之Bagging和Boosting]]></title>
    <url>%2Fmachinelearning%2Fbagging_boosting%2F</url>
    <content type="text"><![CDATA[集成学习(Ensemble Learning)有两个流派，一个是boosting派系，它的特点是各个弱学习器之间有依赖关系。另一种是bagging流派，它的特点是各个弱学习器之间没有依赖关系，可以并行拟合。 Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法。即将弱分类器组装成强分类器的方法。 首先介绍Bootstraping，即自助法：它是一种有放回的抽样方法（可能抽到重复的样本）。 1、Bagging (bootstrap aggregating) Bagging即套袋法，其算法过程如下：A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同） 2、Boosting其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。关于Boosting的两个核心问题：1）在每一轮如何改变训练数据的权值或概率分布？通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。2）通过什么方式来组合弱分类器？通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。 3、Bagging，Boosting二者之间的区别Bagging和Boosting的区别：1）样本选择上：Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。2）样例权重：Bagging：使用均匀取样，每个样例的权重相等Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。3）预测函数：Bagging：所有预测函数的权重相等。Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。4）并行计算：Bagging：各个预测函数可以并行生成Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。 4、总结这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。下面是将决策树与这些算法框架进行结合所得到的新的算法：1）Bagging + 决策树 = 随机森林2）AdaBoost + 决策树 = 提升树3）Gradient Boosting + 决策树 = GBDT 随机森林（RF, Random Forest）原理Random Forest（随机森林）是Bagging的扩展变体，它在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入随机特征选择，因此可以概括RF包括四个部分： 随机选择样本（放回抽样） 随机选择特征 构建决策树 随机森林投票（平均） 随机选择样本和Bagging相同，随机选择特征是指在书的构建中，会从样本集的特征集合中随机选择部分特征，然后再从这个 子集中选择最优的属性用于划分，这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵随机树），但是由于随机森林的‘平均’特性，会使得它的方差减小，而且方差的减小步长了偏差的增大，因此总体而言是更好的模型。 在构建决策树的时候，RF的每棵决策树都最大可能的进行生长而不进行剪枝；在对预测输出进行结合时，RF通常对分类问题使用简单投票法，回归任务使用简单平均法。 RF的重要特性是不用对其进行交叉验证或者使用一个独立的测试集获得无偏估计，它可以在内部进行评估，也就是说在生成的过程中可以对误差进行无偏估计，由于每个基学习器只使用了训练集中约63.2%的样本，剩下约36.8%的样本可用做验证集来对其泛化性能进行“包外估计” 对于一个样本，它在某一次含m个样本的训练集的随机采样中，每次被采集到的概率是$\frac{1}{m}$。不被采集到的概率为$1- \frac{1}{m}$。如果m次采样都没有被采集中的概率是$(1- \frac{1}{m})^m$。当$m \to \infty $时，$(1-\frac{1}{m})^m \to \frac{1}{e} \simeq 0.368$。也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。对于这部分大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。刘建平博客 RF和Bagging对比：RF的起始性能较差，特别当只有一个基学习器时，随着学习器数据增多，随机森林通常会收敛到更低的泛化误差。随机森林的训练效率也会高于Bagging，因为在单个决策树的构建中，Bagging使用的是‘确定性’决策树，在选择特征划分结点时，要对所有的特征进行考虑，而随机森林使用的是‘随机性’特征数，只需考虑特征的子集。 优缺点随机森林的优点较多，简单总结： 在数据集上表现良好，相对于其他算法有较大的优势（训练速度、预测准确度） 能够处理很高维的数据，并且不用特征选择，而且在训练完成后，给出特征的重要性3 容易做成并行化方法 RF的缺点：在噪声较大的分类或者回归问题上的会过拟合 GBDT提GBDT之前，谈下Boosting，Boosting是一种与Bagging很类似的技术。不论是Boosting还是Bagging，所使用的多个分类器类型都是一致的。但是在Boosting中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练的分类器的性能来进行训练。Boosting是通过关注被已有分类器错分的那些数据来获得新的分类器。由于Boosting分类的结果是基于所有分类器的加权求和结果的，因此Boosting与Bagging不太一样，Bagging中的分类器权值是一样的，而Boosting中的分类器权重并不相等，每个权重代表对应的分类器在上一轮迭代中的成功度。 原理GBDT与传统的Boosting区别较大。GBDT的每一次计算都是为了减少上一次的残差，而为了消除残差，我们可以在残差减小的梯度方向上建立模型，所以说，在Gradient Boost中，每个新模型的建立是为了使得之前的模型的残差往梯度下降的方法，与传统的Boosting中关注正确错误的样本加权有着很大的区别。 在Gradient Boosting算法中，关键就是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART回归树。 GBDT会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树而不是分类树（尽管GBDT调整后也可以用于分类，单不代表GBDT的树为分类树） 优缺点GBDT的性能在RF的基础上又有一步提升，因此其优点也很明显： 它能灵活地处理各种类型的数据 在相对较少的调参时间下，预测的准确度较高 当然，由于它是Boos，因此基学习器之前存在串行关系，难以并行训练数据。 XGBoost原理XGBoost的性能在GBDT上又有一步提升，而其性能也能通过各种比赛管窥一二。坊间对XGBoost最大的认知在于其能够自动地运用CPU的多线程进行并行计算，同时在算法精度上也进行了精度的提高。由于GBDT在合理的参数设置下，旺旺要生成一定数量的树才能达到令人满意的准确率，当数据集较复杂时，模型可能需要几千次迭代运算。但是XGBoost利用并行的CPU更好的解决了这个问题。其实XGBoost和GBDT的差别也较大，这一点也同样体现在其性能表现上，详见XGBoost与GBDT的区别。 区别GBDT和XGBoost区别 传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑回归（分类）或者线性回归 传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数； XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性 shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率） 列抽样。XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过拟合，还能减少计算 对缺失值的处理。对于特征的值有确实的样本，XGBoost还可以自动学习出它的分裂方向 XGBoost工具支持并行。Boosting是一种串行的结构，怎么并行呢？注意XGBoost的并行是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选择增益最大的那个特征去做分裂，那么哥哥特征的增益计算就可以开多个线程进行RF、GBDT、XGBoost面试级整理 刘建平博客. Bagging与随机森林算法原理小结 &#8617; RF、GBDT、XGBoost面试级整理. RF、GBDT、XGBoost面试级整理 &#8617;]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark性能优化]]></title>
    <url>%2Fspark%2FSpark_tuning%2F</url>
    <content type="text"><![CDATA[在使用Spark的过程中，我们通常会受限于集群的资源（比如内存、磁盘或者CPU）。为了追求更好的性能，更简洁的Spark代码，可以从以下几个方面进行实践和优化： 充分利用钨丝计划（Tungsten） 分析执行计划 数据管理（比如持久化、广播） 云相关的优化 [1]. Spark performance tuning from the trenches [2] Spark Tuning for Enterprise System Administrators By Anya Bida [3] Top 5 mistakes when writing Spark applications [4] Cheat Sheet - Spark Performance Tuning [5] Spark Tuning – A Starting Point]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Metric Trees]]></title>
    <url>%2Falgorithm%2FMetric_Trees%2F</url>
    <content type="text"><![CDATA[Metric tree in an indexing structure that allows for efficient KNN search1 Metric tree organizes a set of points hierarchically It’s a binary tree: nodes = sets of points, root = all points sets across siblings (nodes on the same level) are all disjoint at each internal node all points are partitioned into 2 disjoint sets Notation: let $N(v)$ be all points at node $v$ $left(v)$,$right(v)$ - left and right children of $v$ Splitting a node: choose two pivot points $p_l$ and $p_r$ from $N(v)$ ideally these points should be selected s.t. the distance between them is largest: $(pl,p_r)=\arg \max {p_l,p_r\in N(v)}\left | p_1-p_2 \right |$ but it takes $O(n^2)$(where $n=\left | N(v) \right |$) to find optimal $p_l, p_r$ heuristic: pick a random point $p \in N(v)$ then let $p_l$ be point farthest from $p$ and then let $p_r$ be point farthest from $p_l$ once we have $(p_l, p_r)$ we can partition: project all points onto a line $u=p_r-p_l$ find the median point $A$ along the line $u$ all points on the left of $A$ got to $left(v)$, on the right of $A$ - to $right(v)$ by using the median we ensure that the depth of the tree is $O(\log N)$ where $N$ is the total number of data points however finding the median is expensive heuristic: can use the mean point as well, i.e. $A=(p_l+p_r)/2$ let $L$ be a $d-1$ dimensional plane orthogonal to $u$ that goes through $A$ this $L$ is a decision boundary - we will use it for querying After metric tree is constructed at each node we have: the decision boundary $L$ a sphere $\mathbb B$ s.t. all points in $N(v)$ are in this sphere let $center(v)$ be the center of $\mathbb B$ and $r(v)$ be the radius so $N(v)\subseteqq \mathbb B(center(v), r(v))$ MT-DFS($q$) - the search algorithm search in a Metric Tree is a guided Depth-First Search the decision boundary $L$ at each node $n$ is used to decide whether to go left or right if $q$ is in the left , then go to $left(v)$, otherwise - to $right(v)$ (or can project the query point to $u$, and then check if $q&lt; A$ or not) all the time we maintain $x$: nearest neighbor found so far let $d=\left | x-q \right |$ - distance from best $x$ so far to the query we can use $d$ to prune nodes: we can check if node is good or no point can better than $x$ no point is better than $x$ if $\left | center(r)-q \right |-r(v)\geqslant d$. That means if the hyper-sphere intersects with current candidates sphere（判断超球与当前查询点的超球是否无交集，两球心距离是否大于等两半径和：$\left | center(v)-q \right | \geqslant r(v) + \left | x-q\right |$，即满足该条件时，两个超球体相交或者相离） This algorithm is very efficient when dimensionality is $\leqslant 30$ but slows down when it increases Observation: MT often finds the NN very quickly and then spends 95% of the time verifying that this is the true NN can reduce this time with Spill-Tree 1.https://www.quora.com/What-is-a-kd-tree-and-what-is-it-used-for ↩]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark多Job并发执行]]></title>
    <url>%2Fspark%2FSpark_multi-job%2F</url>
    <content type="text"><![CDATA[在使用spark处理数据的时候，大多数都是提交一个job执行，然后job内部会根据具体的任务，生成task任务，运行在多个进程中，比如读取的HDFS文件的数据，spark会加载所有的数据，然后根据block个数生成task数目，多个task运行中不同的进程中，是并行的，如果在同一个进程中一个JVM里面有多个task，那么多个task也可以并行，这是常见的使用方式。 考虑下面一种场景，在HDFS上某个目录下面有10个文件，我想要同时并行的去统计每个文件的数量，应该怎么做？ 其实spark是支持在一个spark context中可以通过多线程同时提交多个任务运行，然后spark context接到这所有的任务之后，通过中央调度，在来分配执行各个task，最终任务完成程序退出。 下面就来看下如何使用多线程提交任务，可以直接使用new Thread来创建线程提交，但是不建议这么做，推荐的做法是通过Executors线程池来异步管理线程，尤其是在提交的任务比较多的时候用这个会更加方便。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.concurrent.&#123;Callable, Executors, Future&#125;import org.apache.spark.sql.SparkSessionimport scala.collection.mutable.ArrayBufferobject MultiThread &#123; def main(args: Array[String]) &#123; val spark = SparkSession .builder() .appName("Spark Multi thread") .getOrCreate() val sc = spark.sparkContext //保存任务返回值 val list = ArrayBuffer[Future[String]]() //并行任务读取的path val task_paths = ArrayBuffer[String]() task_paths.+=("path1") task_paths.+=("path2") task_paths.+=("path3") //线程数等于path的数量 val nums_threads = task_paths.length //构建线程池 val executors = Executors.newFixedThreadPool(nums_threads) for (i &lt;- 0 until nums_threads) &#123; val task = executors.submit(new Callable[String] &#123; override def call(): String = &#123; val count: Long = sc.textFile(task_paths.apply(i)).count() //获取统计文件数量 task_paths.apply(i) + " 文件数量： " + count &#125; &#125;) list += task //添加集合里面 &#125; executors.shutdown() //遍历获取结果 list.foreach(result =&gt; println(result.get())) //停止spark // spark.stop() &#125;&#125; 参考Spark如何在一个SparkContext中提交多个任务Spark优化(1)-多Job并发执行]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【牛人博客之】梯度下降（Gradient Descent）]]></title>
    <url>%2Fmachinelearning%2Fgradient_descent%2F</url>
    <content type="text"><![CDATA[在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。 梯度在微积分里面，对多元函数的参数求$\partial$偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数$f(x,y)$, 分别对$x,y$求偏导数，求得的梯度向量就是$(\partial f / \partial x, \partial f / \partial y)^T$，简称$grad \ f(x,y)$或者$\bigtriangledown f(x,y)$。对于在点$(x0,y0)$的具体梯度向量就是$(\partial f / \partial x, \partial f / \partial y)^T$.或者$\bigtriangledown f(x,y)$，如果是3个参数的向量梯度，就是$(\partial f / \partial x, \partial f / \partial y , \partial f / \partial z )^T$,以此类推。 那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数$f(x,y)$,在点$(x0,y0)$，沿着梯度向量的方向就是$(\partial f/\partial x_0, \partial f/\partial y_0)^T$的方向是$f(x,y)$增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是$-(\partial f/\partial x_0, \partial f/\partial y_0)^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。 梯度下降的相关概念在详细了解梯度下降的算法之前，我们先看看相关的一些概念。 步长（Learning rate）：步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。 特征（feature）：指的是样本中输入部分，比如2个单特征的样本$(x^{(0)}, y^{(0)}),(x^{(1)}, y^{(1)})$,则第一个样本特征为$x^{(0)}$，第一个样本输出为$y^{(0)}$。 假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_{\theta}(x)$。比如对于单个特征的m个样本$(x^{(i)}, y^{(i)}) (i=1,2,…m)$,可以采用拟合函数如下： $h_{\theta}(x) = \theta_0+\theta_1x$。 损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于m个样本$(x_i,y_i)(i=1,2,…m)$,采用线性回归，损失函数为：J(\theta_0, \theta_1) = \sum\limits_{i=1}^{m}(h_\theta(x_i) - y_i)^2其中$x_i$表示第i个样本特征，$y_i$表示第$i$个样本对应的输出，$h_{\theta}(x_i)$为假设函数。 梯度下降的算法调优在使用梯度下降时，需要进行调优。哪些地方需要调优呢？ 算法的步长选择。在前面的算法描述中，步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。 算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。 归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的期望$\overline{x}$和标准差$std(x)$，然后转化为：\frac{x - \overline{x}}{std(x)}这样特征的新期望为0，新方差为1，迭代次数可以大大加快。 梯度下降法大家族（BGD，SGD，MBGD）批量梯度下降法（Batch Gradient Descent）批量梯度下降法，是梯度下降法最常用的形式，具体做法也就是在更新参数时使用所有的样本来进行更新\theta_i = \theta_i - \alpha\sum\limits_{j=0}^{m}(h_\theta(x_0^{(j)}, x_1^{(j)}, ...x_n^{(j)}) - y_j)x_i^{(j)}这里求梯度的时候就用了所有m个样本的梯度数据。 随机梯度下降法（Stochastic Gradient Descent）随机梯度下降法，其实和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本j来求梯度。对应的更新公式是：\theta_i = \theta_i - \alpha (h_\theta(x_0^{(j)}, x_1^{(j)}, ...x_n^{(j)}) - y_j)x_i^{(j)}随机梯度下降法，和批量梯度下降法是两个极端，一个采用所有数据来梯度下降，一个用一个样本来梯度下降。自然各自的优缺点都非常突出。对于训练速度来说，随机梯度下降法由于每次仅仅采用一个样本来迭代，训练速度很快，而批量梯度下降法在样本量很大的时候，训练速度不能让人满意。对于准确度来说，随机梯度下降法用于仅仅用一个样本决定梯度方向，导致解很有可能不是最优。对于收敛速度来说，由于随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解。 小批量梯度下降法（Mini-batch Gradient Descent）小批量梯度下降法是批量梯度下降法和随机梯度下降法的折衷，也就是对于m个样本，我们采用x个样子来迭代，1&lt;x&lt;m。一般可以取x=10，当然根据样本的数据，可以调整这个x的值。对应的更新公式是：\theta_i = \theta_i - \alpha \sum\limits_{j=t}^{t+x-1}(h_\theta(x_0^{(j)}, x_1^{(j)}, ...x_n^{(j)}) - y_j)x_i^{(j)} 梯度下降法和其他无约束优化算法的比较在机器学习中的无约束优化算法，除了梯度下降以外，还有前面提到的最小二乘法，此外还有牛顿法和拟牛顿法。 梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。 梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【牛人博客之】DBScan]]></title>
    <url>%2Fmachinelearning%2FDBScan%2F</url>
    <content type="text"><![CDATA[基于密度的方法(Density-based methods)基本思想基于密度的方法：k-means解决不了不规则形状的聚类。于是就有了Density-based methods来系统解决这个问题。该方法同时也对噪声数据的处理比较好。其原理简单说画圈儿，其中要定义两个参数，一个是圈儿的最大半径，一个是一个圈儿里最少应容纳几个点。只要邻近区域的密度（对象或数据点的数目）超过某个阈值，就继续聚类,最后在一个圈里的，就是一个类各种聚类算法。DBSCAN（Density-Based Spatial Clustering of Applications with Noise）dbscan论文就是其中的典型. DBSCAN密度定义刘建平博客DBSCAN是基于一组邻域来描述样本集的紧密程度的，参数($\epsilon$, MinPts)用来描述邻域的样本分布紧密程度。其中，$\epsilon$描述了某一样本的邻域距离阈值 （即以该样本为圆心的超球半径），MinPts描述了某一样本的距离为ϵ的邻域中样本个数的阈值（即落在该超球体内的样本点个数）。 假设我的样本集是$D=(x_1,x_2,…,x_m)$,则DBSCAN具体的密度描述定义如下： $\epsilon$-邻域：对于$xj \in D$，其$\epsilon$-邻域包含样本集$D$中与$x_j$的距离不大于$\epsilon$的子样本，即$N{\epsilon}(xj)={ x_i \in D|distance(x_i, x_j) \leq \epsilon}$，这个子样本集的个数记为$|N{\epsilon}(x_j)|$。以$x_j$为中心，以$\epsilon$为半径的超球体即为$\epsilon$-邻域 核心对象：对于任一样本$xj \in D$，如果其$\epsilon$-邻域对应的$N{\epsilon}(xj)$至少包含MinPts个样本，即如果$|N{\epsilon}(x_j)| \leq MinPts$，则$x_j$是核心对象。满足阈值门限的超球体中心为核心对象 密度直达：如果$x_i$位于$x_j$的$\epsilon$-邻域中，且$x_j$是核心对象，则称$x_i$由$x_j$密度直达。注意反之不一定成立，即此时不能说$x_j$由$x_i$密度直达, 除非且$x_i$也是核心对象。被包含在以$x_j$为中心的超球体中所有实例点都是由$x_j$密度直达 密度可达：对于$xi$和$x_j$,如果存在样本样本序列$p_1,p_2,…,p_T$,满足$p_1=x_i,p_T=x_j$, 且$p{t+1}$由$pt$密度直达，则称$x_j$由$x_i$密度可达。也就是说，密度可达满足传递性。此时序列中的传递样本$p_1,p_2,…,p{T-1}$均为核心对象，因为只有核心对象才能使其他样本密度直达。注意密度可达也不满足对称性，这个可以由密度直达的不对称性得出。 密度相连：对于$x_i$和$x_j$,如果存在核心对象样本$x_k$，使$x_i$和$x_j$均由$x_k$密度可达，则称$x_i$和$x_j$密度相连。注意密度相连关系是满足对称性的。 从下图可以很容易看出理解上述定义，图中$MinPts=5$，红色的点都是核心对象，因为其$\epsilon$-邻域至少有5个样本。黑色的样本是非核心对象。所有核心对象密度直达的样本在以红色核心对象为中心的超球体内，如果不在超球体内，则不能密度直达。图中用绿色箭头连起来的核心对象组成了密度可达的样本序列。在这些密度可达的样本序列的$\epsilon$-邻域内所有的样本相互都是密度相连的。 DBSCAN密度聚类思想DBSCAN的聚类定义很简单：由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别，或者说一个簇。 这个DBSCAN的簇里面可以有一个或者多个核心对象。如果只有一个核心对象，则簇里其他的非核心对象样本都在这个核心对象的$\epsilon$-邻域里；如果有多个核心对象，则簇里的任意一个核心对象的$\epsilon$-邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。这些核心对象的$\epsilon$-邻域里所有的样本的集合组成的一个DBSCAN聚类簇。 那么怎么才能找到这样的簇样本集合呢？DBSCAN使用的方法很简单，它任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够密度可达的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找密度可达的样本集合，这样就得到另一个聚类簇。一直运行到所有核心对象都有类别为止。 基本上这就是DBSCAN算法的主要内容了，是不是很简单？但是我们还是有三个问题没有考虑。 第一个是一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN中，我们一般将这些样本点标记为噪音点。 第二个是距离的度量问题，即如何计算某样本和核心对象样本的距离。在DBSCAN中，一般采用最近邻思想，采用某一种距离度量来衡量样本距离，比如欧式距离。这和KNN分类算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用KD树或者球树来快速的搜索最近邻。 第三种问题比较特殊，某些样本$x_a$可能到两个核心$x_b, x_c$对象的距离都小于$\epsilon$，但是这两个核心对象由于不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？一般来说，此时DBSCAN采用先来后到，先进行聚类的类别簇会标记这个样本为它的类别。也就是说BDSCAN的算法不是完全稳定的算法。此时$x_a$为非核心对象，且处于两个簇（$x_b$所在簇和$x_c$所在簇）的边缘，如上图中两个簇的交汇处的点 DBSCAN小结和传统的K-Means算法相比，DBSCAN最大的不同就是不需要输入类别数k，当然它最大的优势是可以发现任意形状的聚类簇，而不是像K-Means，一般仅仅使用于凸的样本集聚类。同时它在聚类的同时还可以找出异常点，这点和BIRCH算法类似。 那么我们什么时候需要用DBSCAN来聚类呢？一般来说，如果数据集是稠密的，并且数据集不是凸的，那么用DBSCAN会比K-Means聚类效果好很多。如果数据集不是稠密的，则不推荐用DBSCAN来聚类。（如何判定数据集是稠密的？） 下面对DBSCAN算法的优缺点做一个总结。 DBSCAN的主要优点有： 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。（如何理解聚类结果没有偏倚，在实际应用中如何体现） DBSCAN的主要缺点有： 如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。(如何判定样本集的密度不均匀？聚类间距相差大如何理解？) 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值$\epsilon$，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。 各种聚类算法. 各种聚类算法的系统介绍和比较 &#8617; dbscan论文. Density-Based Spatial Clustering of Applications with Noise &#8617; 刘建平博客. DBSCAN密度聚类算法详解 &#8617;]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>算法</tag>
      </tags>
  </entry>
</search>
